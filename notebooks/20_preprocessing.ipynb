{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085ece24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21fa469f",
   "metadata": {},
   "source": [
    "# 11/18 THIS NOTE BOOK IS OUT DATED: Preprocessing\n",
    "Naive bayes will be the base model, and it will struggle with stop words and common words. For that reason, we need to preprocess the data.\n",
    "\n",
    "preprocessing steps include\n",
    "1. remove end of book\n",
    "2. remove start of book\n",
    "3. cleaning extra newline characters\n",
    "7. tokenize words\n",
    "8. Stemming\n",
    "4. map authors to labels\n",
    "5. tokenize to sentences\n",
    "6. make (few sentence, label) pairs\n",
    "\n",
    "X. remove accented characters to ASCII\n",
    "X. Expand contractions\n",
    "X. Lowercase(?) text\n",
    "X. Convert numbers to words\n",
    "X. Remove numbers\n",
    "X. Remove stop words (not for bert)\n",
    "9. Lemmatization (convert to verb root, singular nouns)(nltk package)(not for bert)\n",
    "10. split data into train, validate, and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c21559",
   "metadata": {},
   "source": [
    "There are a variety of python packages that will help with these steps. They are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a5f039",
   "metadata": {},
   "source": [
    "# Table of packages here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8e5a35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34ba1744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/denny/Documents/mids/w266_NLP/lit-shazam'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('..')\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36a88726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "from src.data.make_dataset import get_book\n",
    "# from data.raw.book_urls import book_urls # not working?\n",
    "book_urls = {'great_gatsby':'https://www.gutenberg.org/cache/epub/64317/pg64317.txt',\n",
    "'the_sun_also_rises':'https://www.gutenberg.org/cache/epub/67138/pg67138.txt',\n",
    "'a_tale_of_two_cities':'https://www.gutenberg.org/cache/epub/98/pg98.txt'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deb507e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.gutenberg.org/cache/epub/64317/pg64317.txt\n",
      "https://www.gutenberg.org/cache/epub/67138/pg67138.txt\n",
      "https://www.gutenberg.org/cache/epub/98/pg98.txt\n"
     ]
    }
   ],
   "source": [
    "data_set = {}\n",
    "for title, url in book_urls.items():\n",
    "    print(url)\n",
    "    data_set[title] = get_book(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a4c655",
   "metadata": {},
   "source": [
    "# 1.0 Remove Bookends\n",
    "Books in project gutenberg have lots of extra text at the end of the text file. Most of this is legalese and terms of use information and this is unneeded for the language modeling.\n",
    "\n",
    "Let's look at some examples and then create a framework to remove it from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3badb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "great_gatsby = data_set['great_gatsby']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e35eea86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 1.F.3, a full refund of\\r\\n        any money paid for a work or a replacement copy, if a defect in the\\r\\n        electronic work is discovered and reported to you within 90 days of\\r\\n        receipt of the work.\\r\\n    \\r\\n    • You comply with all other terms of this agreement for free\\r\\n        distribution of Project Gutenberg™ works.\\r\\n    \\r\\n\\r\\n1.E.9. If you wish to charge a fee or distribute a Project\\r\\nGutenberg™ electronic work or group of works on different terms than\\r\\nare set forth in this agreement, you must obtain permission in writing\\r\\nfrom the Project Gutenberg Literary Archive Foundation, the manager of\\r\\nthe Project Gutenberg™ trademark. Contact the Foundation as set\\r\\nforth in Section 3 below.\\r\\n\\r\\n1.F.\\r\\n\\r\\n1.F.1. Project Gutenberg volunteers and employees expend considerable\\r\\neffort to identify, do copyright research on, transcribe and proofread\\r\\nworks not protected by U.S. copyright law in creating the Project\\r\\nGutenberg™ collection. Despite these efforts, Project Gutenberg™\\r\\nelectro'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "great_gatsby[-9000:-8000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e496e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** END OF THE PROJECT GUTENBERG EBOOK THE GREAT GATSBY ***\n"
     ]
    }
   ],
   "source": [
    "# end pattern\n",
    "print('*** END OF THE PROJECT GUTENBERG EBOOK THE GREAT GATSBY ***')\n",
    "end_gutenberg = '*** END OF THE PROJECT GUTENBERG EBOOK THE GREAT GATSBY ***'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3836bcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the span of the found text is  (277737, 277796)\n",
      "the start of the found text is  277737\n",
      "we should remove everything after the start fo the end of book pattern\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "277737"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r'\\*\\*\\* END OF THE PROJECT GUTENBERG EBOOK [\\w\\d\\s]+ \\*\\*\\*'\n",
    "p = re.compile(pattern)\n",
    "print('the span of the found text is ', re.search(pattern, great_gatsby).span())\n",
    "print('the start of the found text is ', re.search(pattern, great_gatsby).start())\n",
    "print('we should remove everything after the start fo the end of book pattern')\n",
    "re.search(pattern, great_gatsby).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e589df0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all books in project gutenberg end with \n",
    "# *** END OF THE PROJECT GUTENBERG EBOOK {Title} ***\n",
    "# *** END OF THE PROJECT GUTENBERG EBOOK THE GREAT GATSBY ***\n",
    "\n",
    "import re\n",
    "def remove_bookend(book:str)->str:\n",
    "    \"\"\"removes the extra end of the book in project gutenberg\"\"\"\n",
    "    end_of_book_pattern = r'\\*\\*\\* END OF THE PROJECT GUTENBERG EBOOK [\\w\\d\\s]+ \\*\\*\\*'\n",
    "    match = re.search(end_of_book_pattern, book)\n",
    "    if match is None:\n",
    "        print('could not find project gutenberg ending')\n",
    "        raise ValueError\n",
    "    last_character = match.start()\n",
    "    return book[:last_character]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ce3942",
   "metadata": {},
   "source": [
    "### Unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3740f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_book_ending ='This is the end of the book. *** END OF THE PROJECT GUTENBERG EBOOK THE the book title with number 10 ***'\n",
    "test_book_no_ending = remove_bookend(test_book_ending)\n",
    "assert test_book_no_ending == 'This is the end of the book. '"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918a0e57",
   "metadata": {},
   "source": [
    "# 2.0 Remove book start\n",
    "This is a harder problem. \n",
    "We do know that all project gutenberg books have boiler plate starting text, which we can find and remove. However, some books have table of contents, other books have introductions, preambles, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5062ad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_book_start(book:str)->str:\n",
    "    \"\"\"removes the boiler plate beginning part of the book in project gutenberg\"\"\"\n",
    "    start_of_book_pattern = r'\\*\\*\\* START OF THE PROJECT GUTENBERG EBOOK [\\w\\d\\s]+ \\*\\*\\*'\n",
    "    match = re.search(start_of_book_pattern, book)\n",
    "    if match is None:\n",
    "        print('could not find project gutenberg beginning')\n",
    "        raise ValueError\n",
    "    first_character = match.end()\n",
    "    return book[first_character:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613607f5",
   "metadata": {},
   "source": [
    "## Unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4172a77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_book_start = '\\ufeffThe Project Gutenberg eBook of The Great Gatsby        This ebook is for the use of anyone anywhere in the United States and  most other parts of the world at no cost and with almost no restrictions  whatsoever. You may copy it, give it away or re-use it under the terms  of the Project Gutenberg License included with this ebook or online  at www.gutenberg.org. If you are not located in the United States,  you will have to check the laws of the country where you are located  before using this eBook.    Title: The Great Gatsby      Author: F. Scott Fitzgerald    Release date: January 17, 2021 [eBook #64317]    Language: English        *** START OF THE PROJECT GUTENBERG EBOOK THE GREAT GATSBY ***          The Great Gatsby        by      F. Scott Fitzgerald                                 Table of Contents    I  II  III  IV  V  VI  VII  VIII  IX                                    Once again                                    to                                   Zelda      Then wear the go'\n",
    "test_book_no_start = remove_book_start(test_book_start)\n",
    "assert test_book_no_start == '          The Great Gatsby        by      F. Scott Fitzgerald                                 Table of Contents    I  II  III  IV  V  VI  VII  VIII  IX                                    Once again                                    to                                   Zelda      Then wear the go'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcfc7381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\r\\n\\r\\n\\t\\t\\t   The Great Gatsby\\r\\n\\t\\t\\t\\t  by\\r\\n\\t\\t\\t F. Scott Fitzgerald\\r\\n\\r\\n\\r\\n                           Table of Contents\\r\\n\\r\\nI\\r\\nII\\r\\nIII\\r\\nIV\\r\\nV\\r\\nVI\\r\\nVII\\r\\nVIII\\r\\nIX\\r\\n\\r\\n\\r\\n                              Once again\\r\\n                                  to\\r\\n                                 Zelda\\r\\n\\r\\n  Then wear the gold hat, if that will move her;\\r\\n  If you can bounce high, bounce for her too,\\r\\n  Till she cry “Lover, gold-hatted, high-bouncing lover,\\r\\n  I must have you!”\\r\\n\\r\\n  Thomas Parke d’Invilliers\\r\\n\\r\\n\\r\\n          '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gg = remove_book_start(great_gatsby)\n",
    "gg = remove_bookend(gg)\n",
    "gg[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "448d7e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for title, book in data_set.items():\n",
    "    book = remove_book_start(book)\n",
    "    book = remove_bookend(book)\n",
    "    data_set[title] = book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c7d6af",
   "metadata": {},
   "source": [
    "# 3.0 remove unwanted characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0aa3b449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, remove unwanted new line and tab characters from the text\n",
    "for char in [\"\\n\", \"\\r\", \"\\d\", \"\\t\", \"\\s\\s\\s\"]:\n",
    "    gg = gg.replace(char, \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bca0600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'I',\n",
       " 'understood',\n",
       " 'that',\n",
       " 'he',\n",
       " 'meant',\n",
       " 'a',\n",
       " 'great',\n",
       " 'deal',\n",
       " 'more',\n",
       " 'than',\n",
       " 'that.',\n",
       " 'In',\n",
       " 'consequence,',\n",
       " 'I’m',\n",
       " 'inclined',\n",
       " 'to',\n",
       " 'reserve']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove header, footer from gutenberg\n",
    "gg[900:1000].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6514e702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_new_line_tabs(book):\n",
    "    \"\"\"remmove unwanted newlines, tabs, etc from the text\"\"\"\n",
    "    for char in [\"\\n\", \"\\r\", \"\\d\", \"\\t\", \"\\s\"]:\n",
    "        book = book.replace(char, \" \")\n",
    "    return book\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9911da",
   "metadata": {},
   "source": [
    "## Unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b12e58dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello \n",
      " I'm trying\t to show \\d the new tabs \\s \r",
      " and how it gets \n",
      " broken up. \r",
      " Poetic!\n",
      "hello   I'm trying  to show   the new tabs     and how it gets   broken up.   Poetic!\n"
     ]
    }
   ],
   "source": [
    "# unittest\n",
    "test_book = 'hello \\n I\\'m trying\\t to show \\d the new tabs \\s \\r and how it gets \\n broken up. \\r Poetic!'\n",
    "print(test_book)\n",
    "test_book_ans = \"hello   I'm trying  to show   the new tabs     and how it gets   broken up.   Poetic!\"\n",
    "assert test_book_ans == remove_new_line_tabs(test_book)\n",
    "print(remove_new_line_tabs(test_book))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b645d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing great_gatsby\n",
      "processing the_sun_also_rises\n",
      "processing a_tale_of_two_cities\n"
     ]
    }
   ],
   "source": [
    "for title, book in data_set.items():\n",
    "    print(f'processing {title}')\n",
    "    data_set[title] = remove_new_line_tabs(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cd7b06",
   "metadata": {},
   "source": [
    "# 4.0 Map authors to labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a2b807d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Fitzgerald': 0, 'Hemingway': 1, 'Dickens': 2}\n"
     ]
    }
   ],
   "source": [
    "id2author = {0:'Fitzgerald',1:'Hemingway',2:'Dickens'}\n",
    "author2id = {value:key for (key, value) in id2author.items()}\n",
    "print(author2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4134251",
   "metadata": {},
   "source": [
    "# 5.0 Tokenize sentences\n",
    "We'll use the sent_tokenize package from nltk. An alternative is the tf_text tokenizer.\n",
    "\n",
    "The sent_tokenizer takes text to split into sentences and outputs the sentences as a list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e75a6050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/denny/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d08909fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the sentences are of <class 'list'> and there are 2439 many sentences\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(gg)\n",
    "print(f'the sentences are of {type(sentences)} and there are {len(sentences)} many sentences')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839ecc89",
   "metadata": {},
   "source": [
    "# 8.0 Tokenize words\n",
    "We use the nltk tokenizer which comes from the treebank word tokenizer.\n",
    "\n",
    "not sure if this is actually needed, but it is required for stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ded397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 nltk tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b208ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1, s2, s3 = sentences[1], sentences[2], sentences[3]\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ff3250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "tb_tokenizer = tokenize.treebank.TreebankWordTokenizer()\n",
    "token_list = tb_tokenizer.tokenize(s2)\n",
    "print(token_list[0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d58ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 tf textvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c0515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "tv = TextVectorization()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdadc56e",
   "metadata": {},
   "source": [
    "# 9.0 Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba43d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "porter_tokens = [porter.stem(token) for token in token_list]\n",
    "porter_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ff4734",
   "metadata": {},
   "source": [
    "# Remove punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33475e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.0 Lemmatization\n",
    "\n",
    "what it does\n",
    "- nouns: convert plural to singular\n",
    "- verbs: convert \n",
    "\n",
    "why it's useful\n",
    "\n",
    "- normalization technique for tokens\n",
    "- reduces variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898cb43d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fd550a8",
   "metadata": {},
   "source": [
    "# Count vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de740704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d893125",
   "metadata": {},
   "source": [
    "# 6.0 Make (few sentences, label) pairs\n",
    "\n",
    "Our task is `given a few sentences as an example, predict the author` and that means we need to define how many a \"few\" sentences is. We define this as the variable `sentences_per_example` which is an integer between 2-10. We set the default to 3. \n",
    "\n",
    "Then, we will take our \"few sentence example\" and pair that with the author in a tuple of (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fd5587c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "812 813\n"
     ]
    }
   ],
   "source": [
    "sentences_per_example = 3\n",
    "data = []\n",
    "author = 'Fitzgerald'\n",
    "for i in range(int(len(sentences)/sentences_per_example)):\n",
    "    sentence_cluster = sentences[i*sentences_per_example:(i+1)*sentences_per_example] \n",
    "    data += [(sentence_cluster, author)]\n",
    "print(i, len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1fd3c127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['          The Great Gatsby        by      F. Scott Fitzgerald                                 Table of Contents    I  II  III  IV  V  VI  VII  VIII  IX                                    Once again                                    to                                   Zelda      Then wear the gold hat, if that will move her;    If you can bounce high, bounce for her too,    Till she cry “Lover, gold-hatted, high-bouncing lover,    I must have you!”      Thomas Parke d’Invilliers                                        I    In my younger and more vulnerable years my father gave me some advice  that I’ve been turning over in my mind ever since.',\n",
       "  '“Whenever you feel like criticizing anyone,” he told me, “just  remember that all the people in this world haven’t had the advantages  that you’ve had.”    He didn’t say any more, but we’ve always been unusually communicative  in a reserved way, and I understood that he meant a great deal more  than that.',\n",
       "  'In consequence, I’m inclined to reserve all judgements, a  habit that has opened up many curious natures to me and also made me  the victim of not a few veteran bores.'],\n",
       " 0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_to_sentences(book, author_id, sentences_per_example=3):\n",
    "    \"\"\"returns a list of (sentences_per_example, author) pairs \"\"\"\n",
    "    sentences = sent_tokenize(book)\n",
    "    total_clusters = int(len(sentences)/sentences_per_example)\n",
    "    data = []\n",
    "    for i in range(total_clusters):\n",
    "        sentence_cluster = sentences[i*sentences_per_example:(i+1)*sentences_per_example]\n",
    "        data += [(sentence_cluster, author_id)]\n",
    "        \n",
    "    return data\n",
    "author_id = author2id['Fitzgerald']\n",
    "convert_to_sentences(gg, author_id)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9966cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = ('Fitzgerald','Hemingway','Dickens')\n",
    "for title, author in zip(data_set.keys(), authors):\n",
    "    if author == 'Fitzgerald': \n",
    "        author_id = author2id[author]\n",
    "        print(title, author, author_id)\n",
    "        book = data_set[title]\n",
    "        data = convert_to_sentences(book, author_id, sentences_per_example=3)\n",
    "    if title == 'Hemingway': \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fa8426",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac87a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42521c14",
   "metadata": {},
   "source": [
    "# 8.0 Tokenize words\n",
    "We use the nltk tokenizer which comes from the treebank word tokenizer.\n",
    "\n",
    "not sure if this is actually needed, but it is required for stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0764b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1, s2, s3 = data[6][0]\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d394fef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "tb_tokenizer = tokenize.treebank.TreebankWordTokenizer()\n",
    "token_list = tb_tokenizer.tokenize(s2)\n",
    "print(token_list[0:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8501e8",
   "metadata": {},
   "source": [
    "# 9.0 Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86794312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "porter_tokens = [porter.stem(token) for token in token_list]\n",
    "porter_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d586dfb",
   "metadata": {},
   "source": [
    "# 8.0 Lemmatization\n",
    "\n",
    "what it does\n",
    "- nouns: convert plural to singular\n",
    "- verbs: convert \n",
    "\n",
    "why it's useful\n",
    "\n",
    "- normalization technique for tokens\n",
    "- reduces variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53a4b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/python-lemmatization-approaches-with-examples/#\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "# Define function to lemmatize each word with its POS tag\n",
    " \n",
    "# POS_TAGGER_FUNCTION : TYPE 1\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688caccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'the cat is sitting with the bats on the striped mat under many badly flying geese'\n",
    " \n",
    "# tokenize the sentence and find the POS tag for each token\n",
    "pos_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    " \n",
    "print(pos_tagged)\n",
    "#>[('the', 'DT'), ('cat', 'NN'), ('is', 'VBZ'), ('sitting', 'VBG'), ('with', 'IN'), \n",
    "# ('the', 'DT'), ('bats', 'NNS'), ('on', 'IN'), ('the', 'DT'), ('striped', 'JJ'), \n",
    "# ('mat', 'NN'), ('under', 'IN'), ('many', 'JJ'), ('flying', 'VBG'), ('geese', 'JJ')]\n",
    " \n",
    "# As you may have noticed, the above pos tags are a little confusing.\n",
    " \n",
    "# we use our own pos_tagger function to make things simpler to understand.\n",
    "wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "print(wordnet_tagged)\n",
    "#>[('the', None), ('cat', 'n'), ('is', 'v'), ('sitting', 'v'), ('with', None), \n",
    "# ('the', None), ('bats', 'n'), ('on', None), ('the', None), ('striped', 'a'), \n",
    "# ('mat', 'n'), ('under', None), ('many', 'a'), ('flying', 'v'), ('geese', 'a')]\n",
    " \n",
    "lemmatized_sentence = []\n",
    "for word, tag in wordnet_tagged:\n",
    "    if tag is None:\n",
    "        # if there is no available tag, append the token as is\n",
    "        lemmatized_sentence.append(word)\n",
    "    else:        \n",
    "        # else use the tag to lemmatize the token\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "lemmatized_sentence = \" \".join(lemmatized_sentence)\n",
    " \n",
    "print(lemmatized_sentence)\n",
    "#> the cat can be sit with the bat on the striped mat under many fly geese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c194ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # WordNet Lemmatizer\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('wordnet')\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# lemma_tokens = [lemmatizer.lemmatize(token) for token in token_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40de1436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # part of speech tagging\n",
    "# pos_tag = nltk.pos_tag(token_list)\n",
    "# pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c5685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get Lemma Tag Function\n",
    "# def get_lemma_tag(pos_tag):\n",
    "#     if pos_tag.startswith('J'):\n",
    "#         return 'a'\n",
    "#     elif pos_tag.startswith('V'):\n",
    "#         return 'v'\n",
    "#     elif pos_tag.startswith('N'):\n",
    "#         return 'n'\n",
    "#     elif pos_tag.startswith('R'):\n",
    "#         return 'r'\n",
    "#     elif pos_tag.startswith('PRP'):\n",
    "#         return 'n'\n",
    "#     elif pos_tag.startswith('DT'):\n",
    "#         return 'a'\n",
    "#     else:\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fb9d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizer.lemmatize('programming', pos = get_lemma_tag('VBZ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11e52b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizer.lemmatize('of', pos = get_lemma_tag('None'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c632dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('tagsets')\n",
    "# nltk.help.upenn_tagset('IN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100b5bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?lemmatizer.lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f2c85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for token, pos in pos_tag:\n",
    "#     print(token, pos)\n",
    "#     print(get_lemma_tag(pos))\n",
    "#     print(lemmatizer.lemmatize(token, pos=get_lemma_tag(pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecde46da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [lemmatizer.lemmatize(token, pos = get_lemma_tag(pos)) for (token, pos) in pos_tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7ccf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(token_, get_lemma_tag(pos)) for token, (token_, pos) in zip(token_list, pos_tag)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a180900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizer = WordNetLemmatizer()\n",
    "# lemma_tokens = [lemmatizer.lemmatize(token, pos = get_lemma_tag(pos_t)) for (token, (tok, pos_t)) in zip(token_list, nltk.pos_tag(token_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eb3d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1, s2, s3 = data[6][0]\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db49b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "tb_tokenizer = tokenize.treebank.TreebankWordTokenizer()\n",
    "token_list = tb_tokenizer.tokenize(s2)\n",
    "print(token_list[0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838b853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = s2.split()\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7830c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "pos_tags = nltk.pos_tag(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87e4a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "244e9cb2",
   "metadata": {},
   "source": [
    "# Shuffle data into train val test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b03d5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 0.6\n",
    "val_frac = 0.1\n",
    "test_frac = 0.3\n",
    "assert train_frac + val_frac +test_frac == 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d4ceed",
   "metadata": {},
   "source": [
    "## use numpy to shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59ae93cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first record: (['          The Great Gatsby        by      F. Scott Fitzgerald                                 Table of Contents    I  II  III  IV  V  VI  VII  VIII  IX                                    Once again                                    to                                   Zelda      Then wear the gold hat, if that will move her;    If you can bounce high, bounce for her too,    Till she cry “Lover, gold-hatted, high-bouncing lover,    I must have you!”      Thomas Parke d’Invilliers                                        I    In my younger and more vulnerable years my father gave me some advice  that I’ve been turning over in my mind ever since.', '“Whenever you feel like criticizing anyone,” he told me, “just  remember that all the people in this world haven’t had the advantages  that you’ve had.”    He didn’t say any more, but we’ve always been unusually communicative  in a reserved way, and I understood that he meant a great deal more  than that.', 'In consequence, I’m inclined to reserve all judgements, a  habit that has opened up many curious natures to me and also made me  the victim of not a few veteran bores.'], 'Fitzgerald')\n",
      "\n",
      "\n",
      "\n",
      "first record after shuffle: (['Just near the shore along the Sound.”    “What time?”    “Any time that suits you best.”    It was on the tip of my tongue to ask his name when Jordan looked  around and smiled.', '“Having a gay time now?” she inquired.', '“Much better.” I turned again to my new acquaintance.'], 'Fitzgerald')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "print(f'first record: {data[0]}')\n",
    "np.random.shuffle(data)\n",
    "print(f'\\n\\n\\nfirst record after shuffle: {data[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a91ecca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "487 81 245 813\n"
     ]
    }
   ],
   "source": [
    "total = len(data)\n",
    "train = data[0:int(total*train_frac)]\n",
    "val = data[int(total*train_frac):int(total*train_frac)+int(total*val_frac)]\n",
    "test = data[int(total*train_frac)+int(total*val_frac):]\n",
    "\n",
    "print(len(train), len(val), len(test), len(train)+len(val)+len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "492aa84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "487 81 245 813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['That was nineteen-seventeen.',\n",
       "  'By the next year I had a few beaux  myself, and I began to play in tournaments, so I didn’t see Daisy very  often.',\n",
       "  'She went with a slightly older crowd—when she went with anyone  at all.'],\n",
       " 'Fitzgerald')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "def split_train_test_val(train_frac:float=0.6, val_frac:float=0.1, test_frac:float=0.3, data:list[tuple]=[])->tuple[list,list,list]:\n",
    "    \"\"\"takes a list of tuple examples, shuffles them, and splits them into train, val and test data sets based on fractions\n",
    "    The tuples are\n",
    "    ([sentences], label),\n",
    "    ([sentences], label), ...\n",
    "    \n",
    "    Returns 3 lists, \n",
    "        train - list of tuples like ([train sentences], train label)\n",
    "        val - list of tuples like ([validation sentences], validation label) \n",
    "        test - list of tuples like([test sentences], test label) \n",
    "    \"\"\"\n",
    "    assert train_frac + val_frac +test_frac == 1.0, 'tra, val, and test frac must sum to 1'\n",
    "    \n",
    "    # steps\n",
    "    # 1. shuffle data\n",
    "    # 2. split data into train, val and test slices\n",
    "    # 3. return train, val, test\n",
    "    \n",
    "    total_examples = len(data)\n",
    "    \n",
    "    # shuffle the tuples\n",
    "    np.random.shuffle(data)\n",
    "    \n",
    "    # slice random tuples\n",
    "    train = data[0:int(total_examples*train_frac)]\n",
    "    val = data[int(total_examples*train_frac):int(total_examples*train_frac)+int(total_examples*val_frac)]\n",
    "    test = data[int(total_examples*train_frac)+int(total_examples*val_frac):]\n",
    "    \n",
    "    # prints the indecies of the train, val, and test lists. uncomment for debugging\n",
    "    # print(0,int(total_examples*train_frac), int(total_examples*train_frac)+int(total_examples*val_frac), int(total_examples*train_frac)+int(total_examples*val_frac)+int(total_examples*test_frac))\n",
    "    \n",
    "    # print the total tuples in each set\n",
    "    print(len(train), len(val), len(test), len(train)+len(val)+len(test))\n",
    "    \n",
    "    assert len(train)+len(val)+len(test) == total_examples, 'train, val, and test examples are wrong length'\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "train, val, test = split_train_test_val(train_frac, val_frac, test_frac, data)\n",
    "train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f107be",
   "metadata": {},
   "source": [
    "# Preprocess the data\n",
    "\n",
    "1. for each book\n",
    "2. convert text, author to (sentences, labels)\n",
    "3. split to train, val, test lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "435fa22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great_gatsby Fitzgerald 0\n",
      "487 81 245 813\n",
      "the_sun_also_rises Hemingway 1\n",
      "1128 188 564 1880\n",
      "a_tale_of_two_cities Dickens 2\n",
      "1124 187 563 1874\n"
     ]
    }
   ],
   "source": [
    "authors = ('Fitzgerald','Hemingway','Dickens')\n",
    "train = []\n",
    "val = []\n",
    "test = []\n",
    "for title, author in zip(data_set.keys(), authors):\n",
    "    \n",
    "    author_id = author2id[author]\n",
    "    print(title, author, author_id)\n",
    "    book = data_set[title]\n",
    "    data = convert_to_sentences(book, author_id, sentences_per_example=3)\n",
    "    train_, val_, test_ = split_train_test_val(train_frac, val_frac, test_frac, data)\n",
    "    train += train_\n",
    "    val += val_\n",
    "    test += test_\n",
    "    \n",
    " \n",
    " # shuffle when done\n",
    "np.random.shuffle(train)\n",
    "np.random.shuffle(val)\n",
    "np.random.shuffle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a93f4c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['“Is he all right?” Edna asked.',\n",
       "   '“We’d better walk with him.”    “I’m all right,” I said.',\n",
       "   '“Don’t come.'],\n",
       "  1),\n",
       " (['Bill was tired after the bull-fight.',\n",
       "   'So was I.',\n",
       "   'We both took a bull-fight  very hard.'],\n",
       "  1),\n",
       " (['Tomorrow!” Then she added  irrelevantly: “You ought to see the baby.”    “I’d like to.”    “She’s asleep.',\n",
       "   'She’s three years old.',\n",
       "   'Haven’t you ever seen her?”    “Never.”    “Well, you ought to see her.'],\n",
       "  0),\n",
       " (['Whether or  no, the mender of roads ran, on the sultry morning, as if for his life,  down the hill, knee-high in dust, and never stopped till he got to the  fountain.',\n",
       "   'All the people of the village were at the fountain, standing about  in their depressed manner, and whispering low, but showing no other  emotions than grim curiosity and surprise.',\n",
       "   'The led cows, hastily brought  in and tethered to anything that would hold them, were looking stupidly  on, or lying down chewing the cud of nothing particularly repaying their  trouble, which they had picked up in their interrupted saunter.'],\n",
       "  2),\n",
       " (['“_You_ wouldn’t call them bananas?”    “No,” I said.',\n",
       "   '“They’re horns all right.”    “They’re very short,” said Pedro Romero.',\n",
       "   '“Very, very short.'],\n",
       "  1)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.shuffle(train)\n",
    "np.random.shuffle(val)\n",
    "np.random.shuffle(test)\n",
    "train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d3c7ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training examples: 2739\n",
      "validation examples: 456\n",
      "test examples: 1372\n"
     ]
    }
   ],
   "source": [
    "print(f'training examples: {len(train)}\\nvalidation examples: {len(val)}\\ntest examples: {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "82a69e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['“Is he all right?” Edna asked.',\n",
       "  '“We’d better walk with him.”    “I’m all right,” I said.',\n",
       "  '“Don’t come.'],\n",
       " 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b866aff",
   "metadata": {},
   "source": [
    "# Count Vectorize\n",
    "In this section, the words for each example will be converted to a numeric value and stored as a sparse matrix in CSR format (see scipy.sparse.csr_matrix for more info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cacdd404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "731c738c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['denny' 'hi' 'is' 'my' 'name' 'paul']\n",
      "[[0 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "s = 'hi my name is Paul'\n",
    "t = 'hi Paul, my name is Denny'\n",
    "cv = CountVectorizer()\n",
    "\n",
    "X = cv.fit_transform([s,t])\n",
    "print(cv.get_feature_names_out())\n",
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c91cd594",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m train:\n\u001b[0;32m----> 5\u001b[0m     v\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mlen\u001b[39m(v)\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# get vocabulary\n",
    "# vocabulary = \n",
    "v = set()\n",
    "for sent in train:\n",
    "    v.update(set(sent))\n",
    "len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2d9d7de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = [pair[0] for pair in train]\n",
    "train_labels = [pair[1] for pair in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4b0cd934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“Is he all right?” Edna asked.“We’d better walk with him.”    “I’m all right,” I said.“Don’t come.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(train_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6ff3422c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2739x10790 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 110578 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "cv.fit_transform([''.join(sentences) for sentences in train_examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d00516f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['“Is he all right?” Edna asked.',\n",
       " '“We’d better walk with him.”    “I’m all right,” I said.',\n",
       " '“Don’t come.']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = s.split()\n",
    "set(t)\n",
    "train[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b493883a",
   "metadata": {},
   "source": [
    "# save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbc7bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data.pkl', 'wb') as f:\n",
    "    pickle.dump([train, val, test], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253c490f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
