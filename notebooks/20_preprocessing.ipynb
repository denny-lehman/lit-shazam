{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085ece24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21fa469f",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Naive bayes will be the base model, and it will struggle with stop words and common words. For that reason, we need to preprocess the data.\n",
    "\n",
    "preprocessing steps include\n",
    "1. cleaning extra newline characters\n",
    "2. remove accented characters to ASCII\n",
    "3. Expand contractions\n",
    "4. Lowercase(?) text\n",
    "5. Convert numbers to words\n",
    "6. Remove numbers\n",
    "7. Remove stop words (not for bert)\n",
    "8. Lemmatization (convert to verb root, singular nouns)(nltk package)(not for bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c21559",
   "metadata": {},
   "source": [
    "There are a variety of python packages that will help with these steps. They are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a5f039",
   "metadata": {},
   "source": [
    "# Table of packages here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e5a35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ba1744",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a88726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "from src.data.make_dataset import get_book\n",
    "# from data.raw.book_urls import book_urls # not working?\n",
    "book_urls = {'great_gatsby':'https://www.gutenberg.org/cache/epub/64317/pg64317.txt',\n",
    "'the_sun_also_rises':'https://www.gutenberg.org/cache/epub/67138/pg67138.txt',\n",
    "'a_tale_of_two_cities':'https://www.gutenberg.org/cache/epub/98/pg98.txt'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb507e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = {}\n",
    "for title, url in book_urls.items():\n",
    "    print(url)\n",
    "    data_set[title] = get_book(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a4c655",
   "metadata": {},
   "source": [
    "# Bookends\n",
    "Books in project gutenberg have lots of extra text at the end of the text file. Most of this is legalese and terms of use information and this is unneeded for the language modeling.\n",
    "\n",
    "Let's look at some examples and then create a framework to remove it from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3badb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "great_gatsby = data_set['great_gatsby']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35eea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "great_gatsby[-9000:-5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e496e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# end pattern\n",
    "print('*** END OF THE PROJECT GUTENBERG EBOOK THE GREAT GATSBY ***')\n",
    "end_gutenberg = '*** END OF THE PROJECT GUTENBERG EBOOK THE GREAT GATSBY ***'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3836bcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'\\*\\*\\* END OF THE PROJECT GUTENBERG EBOOK [\\w\\d\\s]+ \\*\\*\\*'\n",
    "p = re.compile(pattern)\n",
    "print('the span of the found text is ', re.search(pattern, great_gatsby).span())\n",
    "print('the start of the found text is ', re.search(pattern, great_gatsby).start())\n",
    "print('we should remove everything after the start fo the end of book pattern')\n",
    "re.search(pattern, great_gatsby).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e589df0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all books in project gutenberg end with \n",
    "# *** END OF THE PROJECT GUTENBERG EBOOK {Title} ***\n",
    "# *** END OF THE PROJECT GUTENBERG EBOOK THE GREAT GATSBY ***\n",
    "\n",
    "import re\n",
    "def remove_bookend(book:str)->str:\n",
    "    \"\"\"removes the extra end of the book in project gutenberg\"\"\"\n",
    "    end_of_book_pattern = r'\\*\\*\\* END OF THE PROJECT GUTENBERG EBOOK [\\w\\d\\s]+ \\*\\*\\*'\n",
    "    match = re.search(end_of_book_pattern, book)\n",
    "    if match is None:\n",
    "        print('could not find project gutenberg ending')\n",
    "        raise ValueError\n",
    "    last_character = match.start()\n",
    "    return book[:last_character]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ce3942",
   "metadata": {},
   "source": [
    "### Unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3740f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_book_ending ='This is the end of the book. *** END OF THE PROJECT GUTENBERG EBOOK THE the book title with number 10 ***'\n",
    "test_book_no_ending = remove_bookend(test_book_ending)\n",
    "assert test_book_no_ending == 'This is the end of the book. '"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918a0e57",
   "metadata": {},
   "source": [
    "# Remove book start\n",
    "This is a harder problem. \n",
    "We do know that all project gutenberg books have boiler plate starting text, which we can find and remove. However, some books have table of contents, other books have introductions, preambles, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5062ad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_book_start(book:str)->str:\n",
    "    \"\"\"removes the boiler plate beginning part of the book in project gutenberg\"\"\"\n",
    "    start_of_book_pattern = r'\\*\\*\\* START OF THE PROJECT GUTENBERG EBOOK [\\w\\d\\s]+ \\*\\*\\*'\n",
    "    match = re.search(start_of_book_pattern, book)\n",
    "    if match is None:\n",
    "        print('could not find project gutenberg beginning')\n",
    "        raise ValueError\n",
    "    first_character = match.end()\n",
    "    return book[first_character:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613607f5",
   "metadata": {},
   "source": [
    "## Unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4172a77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_book_start = '\\ufeffThe Project Gutenberg eBook of The Great Gatsby        This ebook is for the use of anyone anywhere in the United States and  most other parts of the world at no cost and with almost no restrictions  whatsoever. You may copy it, give it away or re-use it under the terms  of the Project Gutenberg License included with this ebook or online  at www.gutenberg.org. If you are not located in the United States,  you will have to check the laws of the country where you are located  before using this eBook.    Title: The Great Gatsby      Author: F. Scott Fitzgerald    Release date: January 17, 2021 [eBook #64317]    Language: English        *** START OF THE PROJECT GUTENBERG EBOOK THE GREAT GATSBY ***          The Great Gatsby        by      F. Scott Fitzgerald                                 Table of Contents    I  II  III  IV  V  VI  VII  VIII  IX                                    Once again                                    to                                   Zelda      Then wear the go'\n",
    "test_book_no_start = remove_book_start(test_book_start)\n",
    "assert test_book_no_start == '          The Great Gatsby        by      F. Scott Fitzgerald                                 Table of Contents    I  II  III  IV  V  VI  VII  VIII  IX                                    Once again                                    to                                   Zelda      Then wear the go'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfc7381",
   "metadata": {},
   "outputs": [],
   "source": [
    "gg = remove_book_start(great_gatsby)\n",
    "gg = remove_bookend(gg)\n",
    "gg[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14a44bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for title, book in data_set.items():\n",
    "    book = remove_book_start(book)\n",
    "    book = remove_bookend(book)\n",
    "    data_set[title] = book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23a147e",
   "metadata": {},
   "source": [
    "# remove unwanted characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa3b449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, remove unwanted new line and tab characters from the text\n",
    "for char in [\"\\n\", \"\\r\", \"\\d\", \"\\t\", \"\\s\\s\\s\"]:\n",
    "    gg = gg.replace(char, \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bca0600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove header, footer from gutenberg\n",
    "gg[900:1000].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc74e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_new_line_tabs(book):\n",
    "    \"\"\"remmove unwanted newlines, tabs, etc from the text\"\"\"\n",
    "    for char in [\"\\n\", \"\\r\", \"\\d\", \"\\t\", \"\\s\"]:\n",
    "        book = book.replace(char, \" \")\n",
    "    return book\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3d72bd",
   "metadata": {},
   "source": [
    "## Unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7165b3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unittest\n",
    "test_book = 'hello \\n I\\'m trying\\t to show \\d the new tabs \\s \\r and how it gets \\n broken up. \\r Poetic!'\n",
    "print(test_book)\n",
    "test_book_ans = \"hello   I'm trying  to show   the new tabs     and how it gets   broken up.   Poetic!\"\n",
    "assert test_book_ans == remove_new_line_tabs(test_book)\n",
    "print(remove_new_line_tabs(test_book))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532f1c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "for title, book in data_set.items():\n",
    "    print(f'processing {title}')\n",
    "    data_set[title] = remove_new_line_tabs(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c8af70",
   "metadata": {},
   "source": [
    "# Map authors to labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93659f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2author = {0:'Fitzgerald',1:'Hemingway',2:'Dickens'}\n",
    "author2id = {value:key for (key, value) in id2author.items()}\n",
    "print(author2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84471ec2",
   "metadata": {},
   "source": [
    "# Tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843ab40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09153b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(gg)\n",
    "print(f'the sentences are of {type(sentences)} and there are {len(sentences)} many sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0074b124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649bfbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sentences = 3\n",
    "data = []\n",
    "author = 'Fitzgerald'\n",
    "for i in range(int(len(sentences)/x_sentences)):\n",
    "    sentence_cluster = sentences[i*x_sentences:(i+1)*x_sentences] \n",
    "    data += [(sentence_cluster, author)]\n",
    "print(i, len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543b77bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_sentences(book, author_id, x_sentences=3):\n",
    "    \"\"\"returns x_sentences, author pairs \"\"\"\n",
    "    sentences = sent_tokenize(book)\n",
    "    total_clusters = int(len(sentences)/x_sentences)\n",
    "    data = []\n",
    "    for i in range(total_clusters):\n",
    "        sentence_cluster = sentences[i*x_sentences:(i+1)*x_sentences]\n",
    "        data += [(sentence_cluster, author_id)]\n",
    "        \n",
    "    return data\n",
    "author_id = author2id['Fitzgerald']\n",
    "convert_to_sentences(gg, author_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3011a81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = ('Fitzgerald','Hemingway','Dickens')\n",
    "for title, author in zip(data_set.keys(), authors):\n",
    "    if author == 'Fitzgerald': \n",
    "        author_id = author2id[author]\n",
    "        print(title, author, author_id)\n",
    "        book = data_set[title]\n",
    "        data = convert_to_sentences(book, author_id, x_sentences=3)\n",
    "    if title == 'Hemingway': \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23200683",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8282cb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd6198a",
   "metadata": {},
   "source": [
    "# Shuffle data into train val test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc298297",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 0.6\n",
    "val_frac = 0.1\n",
    "test_frac = 0.3\n",
    "assert train_frac + val_frac +test_frac == 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39456581",
   "metadata": {},
   "source": [
    "## use numpy to shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ea1914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "print(f'first record: {data[0]}')\n",
    "np.random.shuffle(data)\n",
    "print(f'\\n\\n\\nfirst record after shuffle: {data[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76739e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(data)\n",
    "train = data[0:int(total*train_frac)]\n",
    "val = data[int(total*train_frac):int(total*train_frac)+int(total*val_frac)]\n",
    "test = data[int(total*train_frac)+int(total*val_frac):]\n",
    "\n",
    "print(len(train), len(val), len(test), len(train)+len(val)+len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dc6d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "def split_train_test_val(train_frac:float=0.6, val_frac:float=0.1, test_frac:float=0.3, data:list[tuple]=[])->tuple[list,list,list]:\n",
    "    \"\"\"takes a list of tuple examples, shuffles them, and splits them into train, val and test data sets based on fractions\n",
    "    The tuples are\n",
    "    ([sentences], label),\n",
    "    ([sentences], label), ...\n",
    "    \n",
    "    Returns 3 lists, \n",
    "        train - list of tuples like ([train sentences], train label)\n",
    "        val - list of tuples like ([validation sentences], validation label) \n",
    "        test - list of tuples like([test sentences], test label) \n",
    "    \"\"\"\n",
    "    assert train_frac + val_frac +test_frac == 1.0, 'tra, val, and test frac must sum to 1'\n",
    "    \n",
    "    # steps\n",
    "    # 1. shuffle data\n",
    "    # 2. split data into train, val and test slices\n",
    "    # 3. return train, val, test\n",
    "    \n",
    "    total_examples = len(data)\n",
    "    \n",
    "    # shuffle the tuples\n",
    "    np.random.shuffle(data)\n",
    "    \n",
    "    # slice random tuples\n",
    "    train = data[0:int(total_examples*train_frac)]\n",
    "    val = data[int(total_examples*train_frac):int(total_examples*train_frac)+int(total_examples*val_frac)]\n",
    "    test = data[int(total_examples*train_frac)+int(total_examples*val_frac):]\n",
    "    \n",
    "    # prints the indecies of the train, val, and test lists. uncomment for debugging\n",
    "    # print(0,int(total_examples*train_frac), int(total_examples*train_frac)+int(total_examples*val_frac), int(total_examples*train_frac)+int(total_examples*val_frac)+int(total_examples*test_frac))\n",
    "    \n",
    "    # print the total tuples in each set\n",
    "    print(len(train), len(val), len(test), len(train)+len(val)+len(test))\n",
    "    \n",
    "    assert len(train)+len(val)+len(test) == total_examples, 'train, val, and test examples are wrong length'\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "train, val, test = split_train_test_val(train_frac, val_frac, test_frac, data)\n",
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1080af3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c8d1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "author2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd05e386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd980414",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = ('Fitzgerald','Hemingway','Dickens')\n",
    "train = []\n",
    "val = []\n",
    "test = []\n",
    "for title, author in zip(data_set.keys(), authors):\n",
    "    \n",
    "    author_id = author2id[author]\n",
    "    print(title, author, author_id)\n",
    "    book = data_set[title]\n",
    "    data = convert_to_sentences(book, author_id, x_sentences=3)\n",
    "    train_, val_, test_ = split_train_test_val(train_frac, val_frac, test_frac, data)\n",
    "    train += train_\n",
    "    val += val_\n",
    "    test += test_\n",
    "    \n",
    " \n",
    " # shuffle when done\n",
    "np.random.shuffle(train)\n",
    "np.random.shuffle(val)\n",
    "np.random.shuffle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0d0bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(train)\n",
    "np.random.shuffle(val)\n",
    "np.random.shuffle(test)\n",
    "train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e4adbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'training examples: {len(train)}\\nvalidation examples: {len(val)}\\ntest examples: {len(test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd5f82",
   "metadata": {},
   "source": [
    "# save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc166cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data.pkl', 'wb') as f:\n",
    "    pickle.dump([train, val, test], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ec97cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
