{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43q-iwrUkeoC"
   },
   "source": [
    "Notebook to develop test data using NLTK package and Project Gutenberg. The NLTK corpus consists of 18 works of literature including novels, plays, peoms, and the King James Bible. The version in this notebook will use a balanced training dataset selected only from the subset of novels from the corpus. Notebook will store the data.frame objects as parquet format files for retrieval by downstream notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Leq0Htm6kfir",
    "outputId": "05095a1e-a30a-4733-91ae-72eaa1ce0efd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  pip install [options] [-e] <vcs project url> ...\n",
      "  pip install [options] [-e] <local project path> ...\n",
      "  pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: -u\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install pydot --quiet\n",
    "!pip install nltk --quiet\n",
    "!pip install pyarrow -quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bILXCTzolQ9r",
    "outputId": "2d18afc5-ad8f-4bb3-9dd2-f626464f360d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Dragon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dragon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "from nltk.data import find\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import pyarrow as pya\n",
    "import pyarrow.parquet as pq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "boSLc_EvtgSM",
    "outputId": "0ff8b6f1-acfb-457a-d7cb-367aee882d28"
   },
   "outputs": [],
   "source": [
    "DATA_LOC = 'local'\n",
    "\n",
    "# USE ONLY FOR REMOTE DRIVE\n",
    "# Mount a google Drive for persistent store\n",
    "if DATA_LOC == 'remote':\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQoHEfP0VICk"
   },
   "source": [
    "# Load novels from Project Gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "id": "6pLEMRs_VHKF"
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility support function\n",
    "def remove_new_line_tabs(book):\n",
    "    \"\"\"remove unwanted newlines, tabs, etc from the text\"\"\"\n",
    "    for char in [\"\\n\", \"\\r\", \"\\t\", \"\\d\", \"\\s\"]:\n",
    "        book = book.replace(char, \" \")\n",
    "    return book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "id": "_IsfIMk3WCKc"
   },
   "outputs": [],
   "source": [
    "# ########################################################################################\n",
    "# LOAD INDIVIDUAL NOVELS and remove header and  footer info, including title of the book\n",
    "#\n",
    "# Process flow:\n",
    "#   1. load the novel\n",
    "#   2. search for the end of the novel and cut out the footer info using \"split_str\"\n",
    "#   3. of the results from step 2, cut out the header / preamble / table of contents, including title\n",
    "#      The header info has been analyzed per novel and the starting character value is set at the first\n",
    "#      text character for the body of the novel.\n",
    "#   4. pass the body of the novel through the remove_new_line_tabs function to strip out spaces, tabs, ...\n",
    "#   5. append the processed work to the list of novels\n",
    "# ########################################################################################\n",
    "bks_gutenberg = []\n",
    "split_str = '*** END OF THE PROJECT GUTENBERG EBOOK'\n",
    "\n",
    "# ########################\n",
    "# F. Scott Fitzgerald\n",
    "# ########################\n",
    "# the great gatsby, start at 1200\n",
    "r = requests.get(r'https://www.gutenberg.org/cache/epub/64317/pg64317.txt')\n",
    "r_split = r.text.split(split_str,1)[0]\n",
    "book = remove_new_line_tabs(r_split[1200:])\n",
    "bks_gutenberg.append(book)\n",
    "\n",
    "# this side of paradise\n",
    "r = requests.get(r'https://www.gutenberg.org/cache/epub/805/pg805.txt')\n",
    "r_split = r.text.split(split_str,1)[0]\n",
    "book = remove_new_line_tabs(r_split[1700:])\n",
    "bks_gutenberg.append(book)\n",
    "\n",
    "# beautiful and damned\n",
    "r = requests.get(r'https://www.gutenberg.org/cache/epub/9830/pg9830.txt')\n",
    "r_split = r.text.split(split_str,1)[0]\n",
    "book = remove_new_line_tabs(r_split[2410:])\n",
    "bks_gutenberg.append(book)\n",
    "\n",
    "# ########################\n",
    "# Hemingway\n",
    "# ########################\n",
    "# the sun also rises\n",
    "r = requests.get(r'https://www.gutenberg.org/cache/epub/67138/pg67138.txt')\n",
    "r_split = r.text.split(split_str,1)[0]\n",
    "book = remove_new_line_tabs(r_split[2400:])\n",
    "bks_gutenberg.append(book)\n",
    "\n",
    "# Men Without Women\n",
    "r = requests.get(r'https://www.gutenberg.org/cache/epub/69683/pg69683.txt')\n",
    "r_split = r.text.split(split_str,1)[0]\n",
    "book = remove_new_line_tabs(r_split[3800:])\n",
    "bks_gutenberg.append(book)\n",
    "\n",
    "# In Our Time\n",
    "r = requests.get(r'https://www.gutenberg.org/cache/epub/61085/pg61085.txt')\n",
    "r_split = r.text.split(split_str,1)[0]\n",
    "book = remove_new_line_tabs(r_split[1720:])\n",
    "bks_gutenberg.append(book)\n",
    "\n",
    "# ########################\n",
    "# Thomas Hardy\n",
    "# ########################\n",
    "# Mayor of Casterbridge \n",
    "r = requests.get(r'https://www.gutenberg.org/cache/epub/143/pg143.txt')\n",
    "r_split = r.text.split(split_str,1)[0]\n",
    "book = remove_new_line_tabs(r_split[1200:])\n",
    "bks_gutenberg.append(book)\n",
    "\n",
    "# Jude the Obscure \n",
    "r = requests.get(r'https://www.gutenberg.org/cache/epub/153/pg153.txt')\n",
    "r_split = r.text.split(split_str,1)[0]\n",
    "book = remove_new_line_tabs(r_split[3730:])\n",
    "bks_gutenberg.append(book)\n",
    "\n",
    "# Return of the Native\n",
    "r = requests.get(r'https://www.gutenberg.org/cache/epub/122/pg122.txt')\n",
    "r_split = r.text.split(split_str,1)[0]\n",
    "book = remove_new_line_tabs(r_split[3500:])\n",
    "bks_gutenberg.append(book)\n",
    "\n",
    "# ########################\n",
    "# Dickens\n",
    "# ########################\n",
    "# a tale of two cities\n",
    "r = requests.get(r'https://www.gutenberg.org/cache/epub/98/pg98.txt')\n",
    "r_split = r.text.split(split_str,1)[0]\n",
    "book = remove_new_line_tabs(r_split[2700:])\n",
    "bks_gutenberg.append(book)\n",
    "\n",
    "# Great Expectations\n",
    "r = requests.get(r'https://www.gutenberg.org/cache/epub/1400/pg1400.txt')\n",
    "r_split = r.text.split(split_str,1)[0]\n",
    "book = remove_new_line_tabs(r_split[1850:])\n",
    "bks_gutenberg.append(book)\n",
    "\n",
    "# Bleak House\n",
    "r = requests.get(r'https://www.gutenberg.org/cache/epub/1023/pg1023.txt')\n",
    "r_split = r.text.split(split_str,1)[0]\n",
    "book = remove_new_line_tabs(r_split[2970:])\n",
    "bks_gutenberg.append(book)\n",
    "\n",
    "# ########################\n",
    "# Jane Austen\n",
    "# ########################\n",
    "# Emma\n",
    "r = requests.get(r'https://www.gutenberg.org/cache/epub/158/pg158.txt')\n",
    "r_split = r.text.split(split_str,1)[0]\n",
    "book = remove_new_line_tabs(r_split[1610:])\n",
    "bks_gutenberg.append(book)\n",
    "\n",
    "# Sense\n",
    "r = requests.get(r'https://www.gutenberg.org/cache/epub/161/pg161.txt')\n",
    "r_split = r.text.split(split_str,1)[0]\n",
    "book = remove_new_line_tabs(r_split[1590:])\n",
    "bks_gutenberg.append(book)\n",
    "\n",
    "# Pride\n",
    "r = requests.get(r'https://www.gutenberg.org/cache/epub/1342/pg1342.txt')\n",
    "r_split = r.text.split(split_str,1)[0]\n",
    "book = remove_new_line_tabs(r_split[2550:])\n",
    "bks_gutenberg.append(book)\n",
    "\n",
    "# ########################\n",
    "# Chesterton\n",
    "# ########################\n",
    "# Wisdon of Father Brown\n",
    "r = requests.get(r'https://www.gutenberg.org/cache/epub/223/pg223.txt')\n",
    "r_split = r.text.split(split_str,1)[0]\n",
    "book = remove_new_line_tabs(r_split[1400:])\n",
    "bks_gutenberg.append(book)\n",
    "\n",
    "# The Man Who Was Thursday\n",
    "r = requests.get(r'https://www.gutenberg.org/cache/epub/1695/pg1695.txt')\n",
    "r_split = r.text.split(split_str,1)[0]\n",
    "book = remove_new_line_tabs(r_split[2570:])\n",
    "bks_gutenberg.append(book)\n",
    "\n",
    "# The Ball and the Cross\n",
    "r = requests.get(r'https://www.gutenberg.org/cache/epub/5265/pg5265.txt')\n",
    "r_split = r.text.split(split_str,1)[0]\n",
    "book = remove_new_line_tabs(r_split[1430:])\n",
    "bks_gutenberg.append(book)\n",
    "\n",
    "# ########################\n",
    "# Shakespeare\n",
    "# ########################\n",
    "# As You Like It\n",
    "r = requests.get(r'https://www.gutenberg.org/cache/epub/1786/pg1786.txt')\n",
    "r_split = r.text.split(split_str,1)[0]\n",
    "book = remove_new_line_tabs(r_split[11780:])\n",
    "bks_gutenberg.append(book)\n",
    "\n",
    "# Caesar\n",
    "r = requests.get(r'https://www.gutenberg.org/cache/epub/2263/pg2263.txt')\n",
    "r_split = r.text.split(split_str,1)[0]\n",
    "book = remove_new_line_tabs(r_split[4950:])\n",
    "bks_gutenberg.append(book)\n",
    "\n",
    "# Hamlet\n",
    "r = requests.get(r'https://www.gutenberg.org/cache/epub/2265/pg2265.txt')\n",
    "r_split = r.text.split(split_str,1)[0]\n",
    "book = remove_new_line_tabs(r_split[4900:])\n",
    "bks_gutenberg.append(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rT_-qXSbv0xS",
    "outputId": "a5675975-589d-43c6-b9d2-903a041f90ce"
   },
   "outputs": [],
   "source": [
    "# UNIT TEST\n",
    "#bks_gutenberg[18]\n",
    "#atotc = requests.get(r'https://www.gutenberg.org/cache/epub/98/pg98.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SozgK8MNy-pj"
   },
   "outputs": [],
   "source": [
    "#UNIT TEST\n",
    "#print(bks_gutenberg[1][:-2000])\n",
    "substr = 'START OF THE PROJECT GUTENBERG EBOOK'\n",
    "new_str = bks_gutenberg[1].split(substr,1)[-1]\n",
    "\n",
    "substr2 = 'END OF THE PROJECT GUTENBERG EBOOK'\n",
    "new_str2 = new_str.split(substr2,1)[0]\n",
    "print(new_str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "id": "88kyCQJ_Gz2i"
   },
   "outputs": [],
   "source": [
    "# ######################################################\n",
    "# OLD OLD OLD OLD OLD ... DO NOT CALL\n",
    "# ######################################################\n",
    "bks_gutenberg_processed = []\n",
    "\n",
    "# Process data and get sentence counts\n",
    "start_of_ebook = 'START OF THE PROJECT GUTENBERG EBOOK'\n",
    "end_of_ebook   = 'END OF THE PROJECT GUTENBERG EBOOK'\n",
    "\n",
    "# Clean up header and footer info\n",
    "for indx in range(len(bks_gutenberg)):\n",
    "  #new_text = bks_gutenberg[indx].split(start_of_ebook,1)[-1]\n",
    "  #new_text = new_text.split(end_of_ebook,1)[0]\n",
    "\n",
    "  #for char in [\"\\n\", \"\\r\", \"\\t\"]:\n",
    "  #  new_text = new_text.replace(char, \" \")\n",
    "  new_text = remove_new_line_tabs(new_text)\n",
    "\n",
    "  bks_gutenberg_processed.append(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "id": "j0ivmCv6H4I6"
   },
   "outputs": [],
   "source": [
    "# #################################################################\n",
    "# Combine the 3 novels per author under one concatenated string\n",
    "# #################################################################\n",
    "bks_gutenberg_combined = [' '.join(bks_gutenberg[i:i+3]) for i in range(0, len(bks_gutenberg), 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "id": "hZgMYOKCIqbh"
   },
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# Tokenize sentences and create a dataframe\n",
    "#################################################################################\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# ***************************************************************************\n",
    "# Add Project Gutenberg titles to book list, tokenize sentences\n",
    "# ***************************************************************************\n",
    "sens_count = []\n",
    "bks_gutenberg_sentences = []\n",
    "\n",
    "df_books = pd.DataFrame({\n",
    "\n",
    "   'Author':  ['fitzgerald',\n",
    "           'hemingway',\n",
    "           'hardy',\n",
    "           'dickens',\n",
    "           'austen',\n",
    "           'chesterton',\n",
    "           'shakespeare'],\n",
    "   'Short Title': ['gatsby,this side of paradise,beautiful and damned',\n",
    "                'sun also rises,men without women,in our time',\n",
    "                'mayor,jude,native',\n",
    "                'tale,great expectations,bleak house',\n",
    "                'emma,sense,pride',\n",
    "                'wisdom brown,thurday,ball',\n",
    "                'as you like it,caesar,hamlet'],\n",
    "   'Title': ['The Great Gatsby,This Side of Paradise,The Beautiful and the Damned',\n",
    "          'The Sun Also Rises,Men Without Women,In Our Time',\n",
    "          'The Mayor of Casterbridge,Jude the Obscure,Return of the Native',\n",
    "          'A Tale of Two Cities,Great Expectations,Bleak House',\n",
    "          'Emma,Sense and Sensibility,Pride and Prejudice',\n",
    "          'The Wisdom of Father Brown,The Man Who Was Thursday,The Ball and the Cross',\n",
    "          'As You Like It,Julius Caesar,Hamlet']\n",
    "})\n",
    "\n",
    "\n",
    "for indx in range(len(bks_gutenberg_combined)):\n",
    "  # Get sentence count\n",
    "  # returns each sentence as a list of word strings\n",
    "  sentences = sent_tokenize(bks_gutenberg_combined[indx])\n",
    "  group_sentences = [' '.join(sentence) for sentence in sentences]\n",
    "  sens_count.append(len(group_sentences))\n",
    "  #bks_gutenberg_processed.append(new_text)\n",
    "  bks_gutenberg_sentences.append(sentences)\n",
    "\n",
    "df_books['Sentence Count'] = sens_count\n",
    "bks_gutenberg_sentences = [[string] for string in bks_gutenberg_sentences]\n",
    "\n",
    "#for book in bks_gutenberg_processed:\n",
    "#  books.append(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgIeNTaYV9Yk"
   },
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "id": "hHw6mDos3MxE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "less than chunk_size remaining\n",
      "['\"It was a hard fight, but I didn\\'t give  up and I came through!\"']\n",
      "0\n",
      "less than chunk_size remaining\n",
      "['The six    works constituting the series are:      Indiscretions _of_ Ezra Pound      Women and Men _by_ Ford Madox Ford      Elimus _by_ B. C. Windeler       with Designs _by_ D. Shakespear      The Great American Novel       _by_ William Carlos Williams      England _by_ B.M.G.-Adams      In Our Time _by_ Ernest Hemingway       with Portrait _by_ Henry Strater']\n",
      "1\n",
      "less than chunk_size remaining\n",
      "['“Such as they were, of course.”    “My dear Dame Durden,” said Allan, drawing my arm through his, “do  you ever look in the glass?”    “You know I do; you see me do it.”    “And don’t you know that you are prettier than you ever were?”    I did not know that; I am not certain that I know it now.', 'But I know  that my dearest little pets are very pretty, and that my darling is  very beautiful, and that my husband is very handsome, and that my  guardian has the brightest and most benevolent face that ever was  seen, and that they can very well do without much beauty in me—even  supposing—.']\n",
      "3\n",
      "less than chunk_size remaining\n",
      "['[Illustration:                                      THE                                    END                                     ]                       CHISWICK PRESS:--CHARLES WHITTINGHAM AND CO.                    TOOKS COURT, CHANCERY LANE, LONDON.']\n",
      "4\n",
      "less than chunk_size remaining\n",
      "['The tragedie of HAMLET, Prince of Denmarke.']\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# ##################################################\n",
    "# Create sentence groups of size chunk_size\n",
    "# ##################################################\n",
    "chunk_size = 3\n",
    "book_groups = []\n",
    "\n",
    "for i, book in enumerate(bks_gutenberg_sentences):\n",
    "  combined_sents = []\n",
    "\n",
    "  for j in range(0, len(book[0]), chunk_size):\n",
    "        \n",
    "    rem = len(book[0]) - j\n",
    "    \n",
    "    if rem < chunk_size:\n",
    "        print(\"less than chunk_size remaining\")\n",
    "        group = book[0][j:j+rem]\n",
    "        print(group)\n",
    "        print(i)\n",
    "    else:\n",
    "        group = book[0][j:j+chunk_size]\n",
    "        \n",
    "    new_str = \" \".join(group)\n",
    "    combined_sents.append(new_str)\n",
    "\n",
    "  book_groups.append(combined_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "9wRvr1-45uUF",
    "outputId": "df77793d-c042-48f1-9a24-a538ad5b1526",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Short Title</th>\n",
       "      <th>Title</th>\n",
       "      <th>Sentence Count</th>\n",
       "      <th>Sentence Groups</th>\n",
       "      <th>Group Counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fitzgerald</td>\n",
       "      <td>gatsby,this side of paradise,beautiful and damned</td>\n",
       "      <td>The Great Gatsby,This Side of Paradise,The Bea...</td>\n",
       "      <td>15988</td>\n",
       "      <td>[                           I    In my younger...</td>\n",
       "      <td>5330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hemingway</td>\n",
       "      <td>sun also rises,men without women,in our time</td>\n",
       "      <td>The Sun Also Rises,Men Without Women,In Our Time</td>\n",
       "      <td>9166</td>\n",
       "      <td>[                                          BOO...</td>\n",
       "      <td>3056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hardy</td>\n",
       "      <td>mayor,jude,native</td>\n",
       "      <td>The Mayor of Casterbridge,Jude the Obscure,Ret...</td>\n",
       "      <td>18318</td>\n",
       "      <td>[     I. One evening of late summer, before th...</td>\n",
       "      <td>6106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dickens</td>\n",
       "      <td>tale,great expectations,bleak house</td>\n",
       "      <td>A Tale of Two Cities,Great Expectations,Bleak ...</td>\n",
       "      <td>28184</td>\n",
       "      <td>[   CHAPTER I. The Period      It was the best...</td>\n",
       "      <td>9395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>austen</td>\n",
       "      <td>emma,sense,pride</td>\n",
       "      <td>Emma,Sense and Sensibility,Pride and Prejudice</td>\n",
       "      <td>14605</td>\n",
       "      <td>[      VOLUME I          CHAPTER I      Emma W...</td>\n",
       "      <td>4869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>chesterton</td>\n",
       "      <td>wisdom brown,thurday,ball</td>\n",
       "      <td>The Wisdom of Father Brown,The Man Who Was Thu...</td>\n",
       "      <td>10083</td>\n",
       "      <td>[          ONE -- The Absence of Mr Glass     ...</td>\n",
       "      <td>3361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>shakespeare</td>\n",
       "      <td>as you like it,caesar,hamlet</td>\n",
       "      <td>As You Like It,Julius Caesar,Hamlet</td>\n",
       "      <td>6298</td>\n",
       "      <td>[        SCENE:  OLIVER'S house; FREDERICK'S c...</td>\n",
       "      <td>2100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Author                                        Short Title  \\\n",
       "0   fitzgerald  gatsby,this side of paradise,beautiful and damned   \n",
       "1    hemingway       sun also rises,men without women,in our time   \n",
       "2        hardy                                  mayor,jude,native   \n",
       "3      dickens                tale,great expectations,bleak house   \n",
       "4       austen                                   emma,sense,pride   \n",
       "5   chesterton                          wisdom brown,thurday,ball   \n",
       "6  shakespeare                       as you like it,caesar,hamlet   \n",
       "\n",
       "                                               Title  Sentence Count  \\\n",
       "0  The Great Gatsby,This Side of Paradise,The Bea...           15988   \n",
       "1   The Sun Also Rises,Men Without Women,In Our Time            9166   \n",
       "2  The Mayor of Casterbridge,Jude the Obscure,Ret...           18318   \n",
       "3  A Tale of Two Cities,Great Expectations,Bleak ...           28184   \n",
       "4     Emma,Sense and Sensibility,Pride and Prejudice           14605   \n",
       "5  The Wisdom of Father Brown,The Man Who Was Thu...           10083   \n",
       "6                As You Like It,Julius Caesar,Hamlet            6298   \n",
       "\n",
       "                                     Sentence Groups  Group Counts  \n",
       "0  [                           I    In my younger...          5330  \n",
       "1  [                                          BOO...          3056  \n",
       "2  [     I. One evening of late summer, before th...          6106  \n",
       "3  [   CHAPTER I. The Period      It was the best...          9395  \n",
       "4  [      VOLUME I          CHAPTER I      Emma W...          4869  \n",
       "5  [          ONE -- The Absence of Mr Glass     ...          3361  \n",
       "6  [        SCENE:  OLIVER'S house; FREDERICK'S c...          2100  "
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ############################################################################\n",
    "# Store in a dataframe\n",
    "# ############################################################################\n",
    "df_books[\"Sentence Groups\"] = book_groups\n",
    "df_books[\"Group Counts\"] = df_books[\"Sentence Groups\"].apply(lambda x: len(x))\n",
    "df_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Short Title</th>\n",
       "      <th>Title</th>\n",
       "      <th>Sentence Count</th>\n",
       "      <th>Sentence Groups</th>\n",
       "      <th>Group Counts</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fitzgerald</td>\n",
       "      <td>gatsby,this side of paradise,beautiful and damned</td>\n",
       "      <td>The Great Gatsby,This Side of Paradise,The Bea...</td>\n",
       "      <td>15988</td>\n",
       "      <td>[                           I    In my younger...</td>\n",
       "      <td>5330</td>\n",
       "      <td>[After Gatsby’s death the East was haunted for...</td>\n",
       "      <td>[He returned hurriedly to 12 University, left ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hemingway</td>\n",
       "      <td>sun also rises,men without women,in our time</td>\n",
       "      <td>The Sun Also Rises,Men Without Women,In Our Time</td>\n",
       "      <td>9166</td>\n",
       "      <td>[                                          BOO...</td>\n",
       "      <td>3056</td>\n",
       "      <td>[I won’t stand it. Who cares if he is a damn b...</td>\n",
       "      <td>[We walked along. “What did you say that for?”...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hardy</td>\n",
       "      <td>mayor,jude,native</td>\n",
       "      <td>The Mayor of Casterbridge,Jude the Obscure,Ret...</td>\n",
       "      <td>18318</td>\n",
       "      <td>[     I. One evening of late summer, before th...</td>\n",
       "      <td>6106</td>\n",
       "      <td>[And then they had turned from each other in  ...</td>\n",
       "      <td>[“Impudence. Don’t tell folk it was I, mind!” ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dickens</td>\n",
       "      <td>tale,great expectations,bleak house</td>\n",
       "      <td>A Tale of Two Cities,Great Expectations,Bleak ...</td>\n",
       "      <td>28184</td>\n",
       "      <td>[   CHAPTER I. The Period      It was the best...</td>\n",
       "      <td>9395</td>\n",
       "      <td>[Muttering that I would make the inquiry  whet...</td>\n",
       "      <td>[Moreover, he was a boy whom no man could hurt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>austen</td>\n",
       "      <td>emma,sense,pride</td>\n",
       "      <td>Emma,Sense and Sensibility,Pride and Prejudice</td>\n",
       "      <td>14605</td>\n",
       "      <td>[      VOLUME I          CHAPTER I      Emma W...</td>\n",
       "      <td>4869</td>\n",
       "      <td>[“I know little of the game at present,” said ...</td>\n",
       "      <td>[He is an excellent young man, and will suit H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>chesterton</td>\n",
       "      <td>wisdom brown,thurday,ball</td>\n",
       "      <td>The Wisdom of Father Brown,The Man Who Was Thu...</td>\n",
       "      <td>10083</td>\n",
       "      <td>[          ONE -- The Absence of Mr Glass     ...</td>\n",
       "      <td>3361</td>\n",
       "      <td>[The big man in black was staring at me with t...</td>\n",
       "      <td>[He fled frantically down a long lane with his...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>shakespeare</td>\n",
       "      <td>as you like it,caesar,hamlet</td>\n",
       "      <td>As You Like It,Julius Caesar,Hamlet</td>\n",
       "      <td>6298</td>\n",
       "      <td>[        SCENE:  OLIVER'S house; FREDERICK'S c...</td>\n",
       "      <td>2100</td>\n",
       "      <td>[Heere is the Will, and vnder Caesars Seale:  ...</td>\n",
       "      <td>[What makes he here? Did he ask for me? Where ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Author                                        Short Title  \\\n",
       "0   fitzgerald  gatsby,this side of paradise,beautiful and damned   \n",
       "1    hemingway       sun also rises,men without women,in our time   \n",
       "2        hardy                                  mayor,jude,native   \n",
       "3      dickens                tale,great expectations,bleak house   \n",
       "4       austen                                   emma,sense,pride   \n",
       "5   chesterton                          wisdom brown,thurday,ball   \n",
       "6  shakespeare                       as you like it,caesar,hamlet   \n",
       "\n",
       "                                               Title  Sentence Count  \\\n",
       "0  The Great Gatsby,This Side of Paradise,The Bea...           15988   \n",
       "1   The Sun Also Rises,Men Without Women,In Our Time            9166   \n",
       "2  The Mayor of Casterbridge,Jude the Obscure,Ret...           18318   \n",
       "3  A Tale of Two Cities,Great Expectations,Bleak ...           28184   \n",
       "4     Emma,Sense and Sensibility,Pride and Prejudice           14605   \n",
       "5  The Wisdom of Father Brown,The Man Who Was Thu...           10083   \n",
       "6                As You Like It,Julius Caesar,Hamlet            6298   \n",
       "\n",
       "                                     Sentence Groups  Group Counts  \\\n",
       "0  [                           I    In my younger...          5330   \n",
       "1  [                                          BOO...          3056   \n",
       "2  [     I. One evening of late summer, before th...          6106   \n",
       "3  [   CHAPTER I. The Period      It was the best...          9395   \n",
       "4  [      VOLUME I          CHAPTER I      Emma W...          4869   \n",
       "5  [          ONE -- The Absence of Mr Glass     ...          3361   \n",
       "6  [        SCENE:  OLIVER'S house; FREDERICK'S c...          2100   \n",
       "\n",
       "                                               Train  \\\n",
       "0  [After Gatsby’s death the East was haunted for...   \n",
       "1  [I won’t stand it. Who cares if he is a damn b...   \n",
       "2  [And then they had turned from each other in  ...   \n",
       "3  [Muttering that I would make the inquiry  whet...   \n",
       "4  [“I know little of the game at present,” said ...   \n",
       "5  [The big man in black was staring at me with t...   \n",
       "6  [Heere is the Will, and vnder Caesars Seale:  ...   \n",
       "\n",
       "                                                Test  \n",
       "0  [He returned hurriedly to 12 University, left ...  \n",
       "1  [We walked along. “What did you say that for?”...  \n",
       "2  [“Impudence. Don’t tell folk it was I, mind!” ...  \n",
       "3  [Moreover, he was a boy whom no man could hurt...  \n",
       "4  [He is an excellent young man, and will suit H...  \n",
       "5  [He fled frantically down a long lane with his...  \n",
       "6  [What makes he here? Did he ask for me? Where ...  "
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ****************************************************************\n",
    "# PREPARE DATAFRAME\n",
    "#\n",
    "# Random shuffle groups of sentences as a unit, then store the first set\n",
    "# as Training and remaining sentences as Testing based on the split count\n",
    "# Testing. Since we shuffle at first, taking Train, Test sequentially\n",
    "# is still random.\n",
    "# ***************************************************************************\n",
    "\n",
    "# Select Train, Test split\n",
    "train_split = 0.8\n",
    "test_split  = 0.2\n",
    "\n",
    "# Create data structure to put into a dataframe\n",
    "data_train = []\n",
    "data_test  = []\n",
    "\n",
    "for group in book_groups:\n",
    "  #author = authors[i]\n",
    "  #short_title = short_titles[i]\n",
    "  #title = titles[i]\n",
    "\n",
    "  # passages contains the sentences for book i\n",
    "  n = len(group)\n",
    "\n",
    "  train_split_index = int(n*train_split)\n",
    "  test_split_index  = int(n*test_split)\n",
    "\n",
    "  # use temp_group as temp store in order to preserve order in book_group[i]\n",
    "  temp_group = group.copy()\n",
    "  random.shuffle(temp_group)\n",
    "\n",
    "  #train_group = book_groups[i][:train_split_index]\n",
    "  train_group = temp_group[:train_split_index]\n",
    "  test_group  = temp_group[train_split_index:]\n",
    "\n",
    "  data_train.append(train_group)\n",
    "  data_test.append(test_group)\n",
    "\n",
    "df_books[\"Train\"] = data_train\n",
    "df_books[\"Test\"]  = data_test\n",
    "df_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xEtt94XL7vbP"
   },
   "outputs": [],
   "source": [
    "# For remote Drive\n",
    "df_books.to_parquet(\"gutenberg_corpus_df_3chunk.parquet\")\n",
    "#!mv \"nltk_corpus_df_chunks.parquet\" \"/content/drive/My Drive/w266_Project/ProjectStore/gutenberg_corpus_df_3chunk.parquet\"\n",
    "!mv \"gutenberg_corpus_df_3chunk.parquet\" \"/content/drive/My Drive/w266/data/gutenberg_corpus_df_3chunk.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For local drive\n",
    "data_path = 'D:/MIDS/W266/Project/Data/'\n",
    "data_file = 'gutenberg_corpus_df_3chunk_case8.parquet'\n",
    "df_books.to_parquet(data_path+data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3PY2h9RL7zay"
   },
   "outputs": [],
   "source": [
    "# Unit Test parquet file retrieval\n",
    "# read into a pyarrow table\n",
    "# NOTE: list arrays before store get converted to numpy.ndarrays after recalling from Drive\n",
    "table = pya.parquet.read_table(\"/content/drive/My Drive/w266_Project/ProjectStore/nltk_corpus_df_chunks.parquet\")\n",
    "df = table.to_pandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vyr6S6npAtGk"
   },
   "source": [
    "# PREPARE BINARY CLASS DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "id": "pmt6-5V9gSzu"
   },
   "outputs": [],
   "source": [
    "# ##################################################################################################\n",
    "# Function: prepare data for binary classification model\n",
    "#\n",
    "# Binary version data files differ from multiclass by label\n",
    "# ##################################################################################################\n",
    "def create_bin_data(df,index):\n",
    "\n",
    "  train = []\n",
    "  test  = []\n",
    "  list_of_authors = ['fitzgerald','hemingway','hardy','dickens','austen','chesterton','shakespeare']\n",
    "\n",
    "  for indx, row in df.iterrows():\n",
    "    if indx == index:\n",
    "      label_train = [1]*len(row[\"Train\"])\n",
    "      label_test = [1]*len(row[\"Test\"])\n",
    "    else:\n",
    "      label_train = [0]*len(row[\"Train\"])\n",
    "      label_test = [0]*len(row[\"Test\"])\n",
    "\n",
    "    zipped_train = list(zip(row[\"Train\"],label_train))\n",
    "    zipped_test = list(zip(row[\"Test\"],label_test))\n",
    "\n",
    "    train.append(zipped_train)\n",
    "    test.append(zipped_test)\n",
    "\n",
    "  #flatten the list using list comprehension then shuffle\n",
    "  train_shuffled = [item for sublist in train for item in sublist]\n",
    "  random.shuffle(train_shuffled)\n",
    "\n",
    "  test_shuffled = [item for sublist in test for item in sublist]\n",
    "  random.shuffle(test_shuffled)\n",
    "\n",
    "  df_binary_data_train = pd.DataFrame(train_shuffled, columns=['Train Data','Train Label'])\n",
    "  df_binary_data_test  = pd.DataFrame(test_shuffled,  columns=['Test Data' ,'Test Label'])\n",
    "\n",
    "  return(df_binary_data_train, df_binary_data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "id": "X8o0CYcNgw-p"
   },
   "outputs": [],
   "source": [
    "# ############################################################################\n",
    "# Create binary files and store\n",
    "# ############################################################################\n",
    "df_binary_data_train0, df_binary_data_test0 = create_bin_data(df_books,0)\n",
    "df_binary_data_train1, df_binary_data_test1 = create_bin_data(df_books,1)\n",
    "df_binary_data_train2, df_binary_data_test2 = create_bin_data(df_books,2)\n",
    "df_binary_data_train3, df_binary_data_test3 = create_bin_data(df_books,3)\n",
    "df_binary_data_train4, df_binary_data_test4 = create_bin_data(df_books,4)\n",
    "df_binary_data_train5, df_binary_data_test5 = create_bin_data(df_books,5)\n",
    "df_binary_data_train6, df_binary_data_test6 = create_bin_data(df_books,6)\n",
    "\n",
    "if DATA_LOC == 'local':\n",
    "    f_name = 'train_case13_bin.parquet'\n",
    "    f_path = 'D:/MIDS/W266/Project/Data/Bin/'\n",
    "    \n",
    "    df_binary_data_train0.to_parquet(f_path+'train_case13_bin_0.parquet')\n",
    "    df_binary_data_test0.to_parquet(f_path+'test_case13_bin_0.parquet')\n",
    "    \n",
    "    df_binary_data_train1.to_parquet(f_path+'train_case13_bin_1.parquet')\n",
    "    df_binary_data_test1.to_parquet(f_path+'test_case13_bin_1.parquet')\n",
    "    \n",
    "    df_binary_data_train2.to_parquet(f_path+'train_case13_bin_2.parquet')\n",
    "    df_binary_data_test2.to_parquet(f_path+'test_case13_bin_2.parquet')\n",
    "    \n",
    "    df_binary_data_train3.to_parquet(f_path+'train_case13_bin_3.parquet')\n",
    "    df_binary_data_test3.to_parquet(f_path+'test_case13_bin_3.parquet')\n",
    "    \n",
    "    df_binary_data_train4.to_parquet(f_path+'train_case13_bin_4.parquet')\n",
    "    df_binary_data_test4.to_parquet(f_path+'test_case13_bin_4.parquet')\n",
    "    \n",
    "    df_binary_data_train5.to_parquet(f_path+'train_case13_bin_5.parquet')\n",
    "    df_binary_data_test5.to_parquet(f_path+'test_case13_bin_5.parquet')\n",
    "    \n",
    "    df_binary_data_train6.to_parquet(f_path+'train_case13_bin_6.parquet')\n",
    "    df_binary_data_test6.to_parquet(f_path+'test_case13_bin_6.parquet')\n",
    "else:\n",
    "    df_binary_data_train0.to_parquet(\"train_case13_bin_0.parquet\")\n",
    "    df_binary_data_test0.to_parquet(\"test_case13_bin_0.parquet\")\n",
    "    !mv \"train_case13_bin_0.parquet\" \"/content/drive/My Drive/w266/data/train_case13_bin_0.parquet\"\n",
    "    !mv \"test_case13_bin_0.parquet\"  \"/content/drive/My Drive/w266/data/test_case13_bin_0.parquet\"\n",
    "\n",
    "    df_binary_data_train1.to_parquet(\"train_case13_bin_1.parquet\")\n",
    "    df_binary_data_test1.to_parquet(\"test_case13_bin_1.parquet\")\n",
    "    !mv \"train_case13_bin_1.parquet\" \"/content/drive/My Drive/w266/data/train_case13_bin_1.parquet\"\n",
    "    !mv \"test_case13_bin_1.parquet\"  \"/content/drive/My Drive/w266/data/test_case13_bin_1.parquet\"\n",
    "\n",
    "    df_binary_data_train2.to_parquet(\"train_case13_bin_2.parquet\")\n",
    "    df_binary_data_test2.to_parquet(\"test_case13_bin_2.parquet\")\n",
    "    !mv \"train_case13_bin_2.parquet\" \"/content/drive/My Drive/w266/data/train_case13_bin_2.parquet\"\n",
    "    !mv \"test_case13_bin_2.parquet\"  \"/content/drive/My Drive/w266/data/test_case13_bin_2.parquet\"\n",
    "    \n",
    "    df_binary_data_train3.to_parquet(\"train_case13_bin_3.parquet\")\n",
    "    df_binary_data_test3.to_parquet(\"test_case13_bin_3.parquet\")\n",
    "    !mv \"train_case13_bin_3.parquet\" \"/content/drive/My Drive/w266/data/train_case13_bin_3.parquet\"\n",
    "    !mv \"test_case13_bin_3.parquet\"  \"/content/drive/My Drive/w266/data/test_case13_bin_3.parquet\"\n",
    "\n",
    "    df_binary_data_train4.to_parquet(\"train_case13_bin_4.parquet\")\n",
    "    df_binary_data_test4.to_parquet(\"test_case13_bin_4.parquet\")\n",
    "    !mv \"train_case13_bin_4.parquet\" \"/content/drive/My Drive/w266/data/train_case13_bin_4.parquet\"\n",
    "    !mv \"test_case13_bin_4.parquet\"  \"/content/drive/My Drive/w266/data/test_case13_bin_4.parquet\"\n",
    "\n",
    "    df_binary_data_train5.to_parquet(\"train_case13_bin_5.parquet\")\n",
    "    df_binary_data_test5.to_parquet(\"test_case13_bin_5.parquet\")\n",
    "    !mv \"train_case13_bin_5.parquet\" \"/content/drive/My Drive/w266/data/train_case13_bin_5.parquet\"\n",
    "    !mv \"test_case13_bin_5.parquet\"  \"/content/drive/My Drive/w266/data/test_case13_bin_5.parquet\"\n",
    "\n",
    "    df_binary_data_train6.to_parquet(\"train_case13_bin_6.parquet\")\n",
    "    df_binary_data_test6.to_parquet(\"test_case13_bin_6.parquet\")\n",
    "    !mv \"train_case13_bin_6.parquet\" \"/content/drive/My Drive/w266/data/train_case13_bin_6.parquet\"\n",
    "    !mv \"test_case13_bin_6.parquet\"  \"/content/drive/My Drive/w266/data/test_case13_bin_6.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RwixuHB6qCnj",
    "outputId": "67cc8546-cfb4-4459-b523-e7d6c2c1a20f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Both of which,” said Joe, quite charmed  with his logical arrangement, “being done, now this to you a true  friend, say. Namely. You mustn’t go a overdoing on it, but you must  have your supper and your wine and water, and you must be put betwixt  the sheets.”    The delicacy with which Joe dismissed this theme, and the sweet tact  and kindness with which Biddy—who with her woman’s wit had found me out  so soon—had prepared him for it, made a deep impression on my mind.',\n",
       " 2)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "#df_binary_data_test.to_parquet(model_filename)\n",
    "#!mv $model_filename \"/content/drive/My Drive/w266/\"\n",
    "\n",
    "test_shuffled[15]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czWKb-gSAYSM"
   },
   "source": [
    "# PREPARE MULICLASS DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##############################################################\n",
    "# Prepare data for multiclass classification model\n",
    "#\n",
    "# index location --> label\n",
    "# ##############################################################\n",
    "train = []\n",
    "test  = []\n",
    "\n",
    "for indx, row in df_books.iterrows():\n",
    "  #print(len(row[\"Train\"]))\n",
    "  label_train = [indx]*len(row[\"Train\"])\n",
    "  label_test  = [indx]*len(row[\"Test\"])\n",
    "  #print(len(label_train))\n",
    "  zipped_train = list(zip(row[\"Train\"],label_train))\n",
    "  zipped_test = list(zip(row[\"Test\"],label_test))\n",
    "  train.append(zipped_train)\n",
    "  test.append(zipped_test)\n",
    "\n",
    "#flatten the list using list comprehension then shuffle\n",
    "train_shuffled = [item for sublist in train for item in sublist]\n",
    "random.shuffle(train_shuffled)\n",
    "\n",
    "test_shuffled = [item for sublist in test for item in sublist]\n",
    "random.shuffle(test_shuffled)\n",
    "\n",
    "df_multi_data_train = pd.DataFrame(train_shuffled, columns=['Train Data','Train Label'])\n",
    "df_multi_data_test  = pd.DataFrame(test_shuffled,  columns=['Test Data' ,'Test Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FTMjqTeipT-Z"
   },
   "outputs": [],
   "source": [
    "# ############################################\n",
    "# Save to Google Drive\n",
    "# ############################################\n",
    "df_multi_data_train.to_parquet(\"gut_corpus_train_data_multi.parquet\")\n",
    "df_multi_data_valid.to_parquet(\"gut_corpus_valid_data_multi.parquet\")\n",
    "df_multi_data_test.to_parquet(\"gut_corpus_test_data_multi.parquet\")\n",
    "\n",
    "#!mv \"gut_corpus_train_data_multi.parquet\" \"/content/drive/My Drive/w266/gut_corpus_train_data_multi.parquet\"\n",
    "#!mv \"gut_corpus_valid_data_multi.parquet\" \"/content/drive/My Drive/w266/gut_corpus_valid_data_multi.parquet\"\n",
    "#!mv \"gut_corpus_test_data_multi.parquet\" \"/content/drive/My Drive/w266/gut_corpus_test_data_multi.parquet\"\n",
    "!mv \"gut_corpus_train_data_multi.parquet\" \"/content/drive/My Drive/w266/data/gut_corpus_train_data_multi.parquet\"\n",
    "!mv \"gut_corpus_valid_data_multi.parquet\" \"/content/drive/My Drive/w266/data/gut_corpus_valid_data_multi.parquet\"\n",
    "!mv \"gut_corpus_test_data_multi.parquet\" \"/content/drive/My Drive/w266/data/gut_corpus_test_data_multi.parquet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############################################\n",
    "# Save to local drive\n",
    "# ############################################\n",
    "data_file_train = 'train_case8.parquet'\n",
    "#data_file_valid = 'datatest_valid.parquet'\n",
    "data_file_test  = 'test_case8.parquet'\n",
    "data_path = 'D:/MIDS/W266/Project/Data/'\n",
    "df_multi_data_train.to_parquet(data_path+data_file_train)\n",
    "#df_multi_data_valid.to_parquet(data_path+data_file_valid)\n",
    "df_multi_data_test.to_parquet(data_path+data_file_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yos7YJeGAe7v"
   },
   "source": [
    "# PREPARE MULTICLASS BALANCED DATASETS\n",
    "## requires portions of previous sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "id": "J8grg6IV1byl"
   },
   "outputs": [],
   "source": [
    "# ##########################################################################################################\n",
    "# Balance data sets\n",
    "#\n",
    "# we're going to sort the tuple by the second value which is an integer indicating author\n",
    "# then from the sorted data (which should be shuffled in terms of order of sentences from any given novel)\n",
    "# select a max number of sentences not greater than the smallest novel\n",
    "# Then reshuffle and store the training data. Test data can remain at the larger size.\n",
    "# ##########################################################################################################\n",
    "\n",
    "# Find smallest group count, take percentage factor of that amount\n",
    "#MIN_GROUP_COUNT_TRAIN = np.minimum(int(np.min(df_books['Group Counts'])*train_split),100)\n",
    "sample_factor = 0.2\n",
    "MIN_GROUP_COUNT_TRAIN = int(sample_factor * np.min(df_books['Group Counts'])*train_split)\n",
    "MIN_GROUP_COUNT_VALID = int(sample_factor * np.min(df_books['Group Counts'])*valid_split)\n",
    "NUM_OF_LABELS = 7\n",
    "\n",
    "# Sort by label\n",
    "sorted_train = sorted(train_shuffled, key=lambda x: x[1])\n",
    "#sorted_valid = sorted(valid_shuffled, key=lambda x: x[1])\n",
    "\n",
    "train_balanced = []\n",
    "#valid_balanced = []\n",
    "\n",
    "for indx in range(NUM_OF_LABELS):\n",
    "  train_balanced.extend([item for item in sorted_train if item[1] == indx][:MIN_GROUP_COUNT_TRAIN])\n",
    "  #valid_balanced.extend([item for item in sorted_valid if item[1] == indx][:MIN_GROUP_COUNT_VALID])\n",
    "\n",
    "# shuffle labels\n",
    "random.shuffle(train_balanced)\n",
    "#random.shuffle(valid_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0EkgeAUzJbvQ"
   },
   "outputs": [],
   "source": [
    "# ############################################\n",
    "# Save to remote Drive\n",
    "# ############################################\n",
    "df_multi_data_train = pd.DataFrame(train_balanced, columns=['Train Data','Train Label'])\n",
    "df_multi_data_valid = pd.DataFrame(valid_balanced, columns=['Valid Data','Valid Label'])\n",
    "df_multi_data_test  = pd.DataFrame(test_shuffled,  columns=['Test Data' ,'Test Label'])\n",
    "\n",
    "df_multi_data_train.to_parquet(\"gut_corpus_train_data_multi_bal.parquet\")\n",
    "df_multi_data_valid.to_parquet(\"gut_corpus_valid_data_multi_bal.parquet\")\n",
    "df_multi_data_test.to_parquet(\"gut_corpus_test_data_multi_bal.parquet\")\n",
    "\n",
    "!mv \"gut_corpus_train_data_multi_bal.parquet\" \"/content/drive/My Drive/w266/data/gut_corpus_train_data_multi_bal.parquet\"\n",
    "!mv \"gut_corpus_valid_data_multi_bal.parquet\" \"/content/drive/My Drive/w266/data/gut_corpus_valid_data_multi_bal.parquet\"\n",
    "!mv \"gut_corpus_test_data_multi_bal.parquet\" \"/content/drive/My Drive/w266/data/gut_corpus_test_data_multi_bal.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "id": "Ptdrvw1kSEoG"
   },
   "outputs": [],
   "source": [
    "# ############################################\n",
    "# Save to local drive\n",
    "# ############################################\n",
    "df_multi_data_train = pd.DataFrame(train_balanced, columns=['Train Data','Train Label'])\n",
    "#df_multi_data_valid = pd.DataFrame(valid_balanced, columns=['Valid Data','Valid Label'])\n",
    "df_multi_data_test  = pd.DataFrame(test_shuffled,  columns=['Test Data' ,'Test Label'])\n",
    "\n",
    "data_path = 'D:/MIDS/W266/Project/Data/'\n",
    "data_file_train = 'train_bal_case9.parquet'\n",
    "#data_file_valid = 'datatest_valid_bal.parquet'\n",
    "data_file_test  = 'test_case9.parquet'\n",
    "\n",
    "df_multi_data_train.to_parquet(data_path+data_file_train)\n",
    "#df_multi_data_valid.to_parquet(data_path+data_file_valid)\n",
    "df_multi_data_test.to_parquet(data_path+data_file_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.7.2-cp39-cp39-win_amd64.whl (12.2 MB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.27.1)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.8-cp39-cp39-win_amd64.whl (483 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.21.5)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Collecting thinc<8.3.0,>=8.1.8\n",
      "  Downloading thinc-8.2.1-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.8-cp39-cp39-win_amd64.whl (39 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\dragon\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.10.2)\n",
      "Collecting typer<0.10.0,>=0.3.0\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.9-cp39-cp39-win_amd64.whl (122 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.10-cp39-cp39-win_amd64.whl (25 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (61.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.11-cp39-cp39-win_amd64.whl (6.6 MB)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Collecting colorama\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: colorama, catalogue, srsly, murmurhash, cymem, wasabi, typer, smart-open, preshed, confection, cloudpathlib, blis, weasel, thinc, spacy-loggers, spacy-legacy, langcodes, spacy\n",
      "Successfully installed blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 colorama-0.4.6 confection-0.1.4 cymem-2.0.8 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 smart-open-6.4.0 spacy-3.7.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.1 typer-0.9.0 wasabi-1.1.2 weasel-0.3.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script weasel.exe is installed in 'C:\\Users\\Dragon\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script spacy.exe is installed in 'C:\\Users\\Dragon\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "# #########################################\n",
    "# DEVELOP spaCy MODELS\n",
    "# #########################################\n",
    "!pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\dragon\\appdata\\roaming\\python\\python39\\site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (61.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (21.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.64.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\dragon\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\dragon\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\dragon\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.21.5)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.11.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\dragon\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\dragon\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\dragon\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\dragon\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\dragon\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\dragon\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\dragon\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\dragon\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\dragon\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\dragon\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\dragon\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.1.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.9)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\dragon\\appdata\\roaming\\python\\python39\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\dragon\\appdata\\roaming\\python\\python39\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\dragon\\appdata\\roaming\\python\\python39\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\dragon\\appdata\\roaming\\python\\python39\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# THE FOLLOWING spacy MODULES ARE FOR UNIT TESTING ONLY\n",
    "# SEE ...NER version of thie notebook for spacy processing\n",
    "\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(r'https://www.gutenberg.org/cache/epub/64317/pg64317.txt')\n",
    "r_sub = r.text[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(r_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Gutenberg eBook PERSON\n",
      "The Great Gatsby\r\n",
      "    \r\n",
      " WORK_OF_ART\n",
      "the United States GPE\n",
      "the United States GPE\n",
      "eBook PRODUCT\n",
      "Title: The Great Gatsby\r\n",
      "\r\n",
      "\r\n",
      "Author WORK_OF_ART\n",
      "F. Scott Fitzgerald\r\n",
      "\r\n",
      " PERSON\n",
      "January 17, 2021 DATE\n",
      "eBook #64317 LAW\n",
      "English LANGUAGE\n",
      "The Great Gatsby WORK_OF_ART\n",
      "F. Scott Fitzgerald\r\n",
      "\r\n",
      "\r\n",
      "                            PERSON\n"
     ]
    }
   ],
   "source": [
    "#print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "#print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_without_names = ' '.join(['PERSON' if entity.label_ == 'PERSON' else entity.text for entity in doc.ents])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PERSON The Great Gatsby\\r\\n    \\r\\n the United States the United States eBook Title: The Great Gatsby\\r\\n\\r\\n\\r\\nAuthor PERSON January 17, 2021 eBook #64317 English The Great Gatsby PERSON'"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_without_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
