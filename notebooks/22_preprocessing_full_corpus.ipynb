{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30ab4641",
   "metadata": {},
   "source": [
    "# Preprocessing text\n",
    "\n",
    "Input:\n",
    "    \n",
    "    urls to the books\n",
    "    \n",
    "output:\n",
    "\n",
    "    X_train = list of sentences as bag of words\n",
    "    \n",
    "    y_train = author labels (0, 1, 2, ..., n)\n",
    "    \n",
    "    *same for train, test, and validation sets\n",
    "\n",
    "### An example\n",
    "What is an example? ie. X_train[0]?\n",
    "\n",
    "Each example is a bunch of sentences that have been processed into a bag of words such that:\n",
    "\n",
    "`'An example with two sentences. here\\'s the second.'`\n",
    "\n",
    "becomes:\n",
    "\n",
    "`'an exampl with two sentenc here s the second'`\n",
    "\n",
    "\n",
    "### The process\n",
    "What I would like to do:\n",
    "\n",
    "url -> book -> remove unwanted parts -> sentence tokenize -> stem -> reassemble -> pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7a35908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22 Term Frequency inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a711d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f9b10c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/denny/Documents/mids/w266_NLP/lit-shazam'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('..')\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ba065b",
   "metadata": {},
   "source": [
    "`book_urls` contains a dictionary of key:value pairs of book title: web_url to gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e090d6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the first two books:\n",
      " [('The Great Gatsby', 'https://www.gutenberg.org/cache/epub/64317/pg64317.txt'), ('This Side of Paradise', 'https://www.gutenberg.org/cache/epub/805/pg805.txt')]\n"
     ]
    }
   ],
   "source": [
    "from data.raw.book_urls import title2author, book_urls\n",
    "print('the first two books:\\n',list(book_urls.items())[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bf4e17",
   "metadata": {},
   "source": [
    "fetch the data with the `get_book` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ae2e0b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Great Gatsby https://www.gutenberg.org/cache/epub/64317/pg64317.txt\n",
      "This Side of Paradise https://www.gutenberg.org/cache/epub/805/pg805.txt\n",
      "The Beautiful and the Damned https://www.gutenberg.org/cache/epub/9830/pg9830.txt\n",
      "The Sun Also Rises https://www.gutenberg.org/cache/epub/67138/pg67138.txt\n",
      "Men Without Women https://www.gutenberg.org/cache/epub/69683/pg69683.txt\n",
      "In Our Time https://www.gutenberg.org/cache/epub/61085/pg61085.txt\n",
      "time out\n",
      "The Mayor of Casterbridge https://www.gutenberg.org/cache/epub/143/pg143.txt\n",
      "Jude the Obscure https://www.gutenberg.org/cache/epub/153/pg153.txt\n",
      "time out\n",
      "Return of the Native https://www.gutenberg.org/cache/epub/122/pg122.txt\n",
      "A Tale of Two Cities https://www.gutenberg.org/cache/epub/98/pg98.txt\n",
      "Great Expectations https://www.gutenberg.org/cache/epub/1400/pg1400.txt\n",
      "Bleak House https://www.gutenberg.org/cache/epub/1023/pg1023.txt\n",
      "Emma https://www.gutenberg.org/cache/epub/158/pg158.txt\n",
      "Sense and Sensibility https://www.gutenberg.org/cache/epub/161/pg161.txt\n",
      "Pride and Prejudice https://www.gutenberg.org/cache/epub/1342/pg1342.txt\n",
      "The Wisdom of Father Brown https://www.gutenberg.org/cache/epub/223/pg223.txt\n",
      "time out\n",
      "second time out\n",
      "failed to get book, r.status_code = 403\n",
      "The Man Who Was Thursday https://www.gutenberg.org/cache/epub/1695/pg1695.txt\n",
      "The Ball and the Cross https://www.gutenberg.org/cache/epub/5265/pg5265.txt\n",
      "time out\n",
      "As You Like It https://www.gutenberg.org/cache/epub/1786/pg1786.txt\n",
      "Julius Caesar https://www.gutenberg.org/cache/epub/2263/pg2263.txt\n",
      "Hamlet https://www.gutenberg.org/cache/epub/2265/pg2265.txt\n"
     ]
    }
   ],
   "source": [
    "from src.data.make_dataset import get_book\n",
    "import time\n",
    "data_set = {}\n",
    "for title, url in book_urls.items():\n",
    "    print(title,url)\n",
    "    data_set[title] = get_book(url)\n",
    "    time.sleep(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4657f2d9",
   "metadata": {},
   "source": [
    "remove unwanted sections of the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d47b9db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./data/processed/data_set.json', 'w') as f:\n",
    "    json.dump(data_set, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d55d6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./data/processed/data_set.json', 'r') as f:\n",
    "    data_set = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62faf229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/denny/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from src.data.make_dataset import remove_bookend, remove_new_line_tabs, remove_everything_before_starting_sentence\n",
    "from data.raw.book_urls import book_starting_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be78368a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing The Great Gatsby\n",
      "processing This Side of Paradise\n",
      "processing The Beautiful and the Damned\n",
      "processing The Sun Also Rises\n",
      "processing Men Without Women\n",
      "processing In Our Time\n",
      "processing The Mayor of Casterbridge\n",
      "processing Jude the Obscure\n",
      "processing Return of the Native\n",
      "processing A Tale of Two Cities\n",
      "processing Great Expectations\n",
      "processing Bleak House\n",
      "processing Emma\n",
      "processing Sense and Sensibility\n",
      "processing Pride and Prejudice\n",
      "processing The Wisdom of Father Brown\n",
      "processing The Man Who Was Thursday\n",
      "processing The Ball and the Cross\n",
      "processing As You Like It\n",
      "processing Julius Caesar\n",
      "processing Hamlet\n"
     ]
    }
   ],
   "source": [
    "for title, book in data_set.items():\n",
    "    print(f'processing {title}')\n",
    "#     book = remove_book_start(book)\n",
    "    starting_sentence = book_starting_sentence[title]\n",
    "    book = remove_everything_before_starting_sentence(book, starting_sentence)\n",
    "    book = remove_bookend(book)\n",
    "    book = remove_new_line_tabs(book)\n",
    "    data_set[title] = book\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9187ebac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_book' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m failed_book \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe Wisdom of Father Brown\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m title, url \u001b[38;5;241m=\u001b[39m failed_book, book_urls[failed_book]\n\u001b[0;32m----> 3\u001b[0m data_set[title] \u001b[38;5;241m=\u001b[39m \u001b[43mget_book\u001b[49m(url)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_book' is not defined"
     ]
    }
   ],
   "source": [
    "failed_book = 'The Wisdom of Father Brown'\n",
    "title, url = failed_book, book_urls[failed_book]\n",
    "data_set[title] = get_book(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcd89bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8007bbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de66f7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/denny/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11978f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_to_sentences(book, sentences_per_example=3):\n",
    "#     \"\"\"returns a list of (sentences_per_example, author) pairs \"\"\"\n",
    "#     sentences = sent_tokenize(book)\n",
    "#     total_clusters = int(len(sentences)/sentences_per_example)\n",
    "#     data = []\n",
    "#     for i in range(total_clusters):\n",
    "#         sentence_cluster = sentences[i*sentences_per_example:(i+1)*sentences_per_example]\n",
    "#         data += [''.join(sentence_cluster)]\n",
    "        \n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46dd3be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "def sentence_to_bag_of_words(sentence):\n",
    "    \"\"\"Converts words in a sentence into stemmed tokens\"\"\"\n",
    "    # 1. lower case\n",
    "    # 2. remove punctuation\n",
    "    # 3. tokenize\n",
    "    # 4. stem\n",
    "    # 5. TODO: lem\n",
    "    # 6. combine back together with spaces\n",
    "    \n",
    "    result = sentence.lower()\n",
    "    \n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    token_list = tokenizer.tokenize(result)\n",
    "    \n",
    "    \n",
    "    porter = PorterStemmer()\n",
    "    porter_tokens = [porter.stem(token) for token in token_list]\n",
    "    \n",
    "    bag_of_words = ' '.join(porter_tokens)\n",
    "    \n",
    "    return bag_of_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "984dc02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_bag_of_words(examples:list):\n",
    "    return [sentence_to_bag_of_words(batch_of_sentences) for batch_of_sentences in examples]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a02419f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eighti seven mile to go yet onward'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_to_bag_of_words('Eighty-seven miles to go, yet.  Onward!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "608d4cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one exampl with a sentenc here',\n",
       " 'an exampl with two sentenc here s the second',\n",
       " 'a third exampl with some awe inspir exampl of punck tua tion']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_examples = ['One example, with a sentence here!', \n",
    " 'An example with two sentences. here\\'s the second.', \n",
    " 'A third example, with some awe-inspiring Examples of punck:tua-TION!']\n",
    "convert_examples_to_bag_of_words(test_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b12167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fix bug where words like here's are not properly processed\n",
    "# TODO: Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a41f0133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Fitzgerald': 0, 'Hemingway': 1, 'Dickens': 2}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43a0ad3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'The Great Gatsby': 'fitzgerald',\n",
       " 'This Side of Paradise': 'fitzgerald',\n",
       " 'The Beautiful and the Damned': 'fitzgerald',\n",
       " 'The Sun Also Rises': 'hemingway',\n",
       " 'Men Without Women': 'hemingway',\n",
       " 'In Our Time': 'hemingway',\n",
       " 'The Mayor of Casterbridge': 'hardy',\n",
       " 'Jude the Obscure': 'hardy',\n",
       " 'Return of the Native': 'hardy',\n",
       " 'A Tale of Two Cities': 'dickens',\n",
       " 'Great Expectations': 'dickens',\n",
       " 'Bleak House': 'dickens',\n",
       " 'Emma': 'austen',\n",
       " 'Sense and Sensibility': 'austen',\n",
       " 'Pride and Prejudice': 'austen',\n",
       " 'The Wisdom of Father Brown': 'chesterton',\n",
       " 'The Man Who Was Thursday': 'chesterton',\n",
       " 'The Ball and the Cross': 'chesterton',\n",
       " 'As You Like It': 'shakespeare',\n",
       " 'Julius Caesar': 'shakespeare',\n",
       " 'Hamlet': 'shakespeare'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data.raw.book_urls import title2author, author2id\n",
    "from src.data.make_dataset import convert_to_sentences, convert_examples_to_bag_of_words\n",
    "\n",
    "#Encode labels\n",
    "author2id\n",
    "title2author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4967fff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [],[]\n",
    "for title, book in data_set.items():\n",
    "    examples = convert_to_sentences(book, sentences_per_example=3)\n",
    "    processed_examples = convert_examples_to_bag_of_words(examples)\n",
    "    labels = [author2id[title2author[title]]] * len(examples)\n",
    "    X += processed_examples\n",
    "    y += labels\n",
    "    \n",
    "assert len(y) == len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f75ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5db6bff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# train val test split 60%, 20%, 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) # 100% = 80% train, 20% test\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef7446b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'am',\n",
       " 'glad',\n",
       " 'volumnia',\n",
       " 'repeat',\n",
       " 'sir',\n",
       " 'leicest',\n",
       " 'unmind',\n",
       " 'of',\n",
       " 'these']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[3].split()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff9ddad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['then we are both of one mind at last she said ye repli venn gloomili but if you would tell me miss whi you take such an interest in her i should be easier',\n",
       " 'am i sir no jo close hi eye mutter i m weri thank after watch him close a littl while allan put hi mouth veri near hi ear and say to him in a low distinct voic jo did you ever know a prayer never knowd nothink sir not so much as one short prayer no sir nothink at all']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaca560",
   "metadata": {},
   "source": [
    "# Save data\n",
    "X's = ['sit on tom s lap mr wilson call up sever peopl',\n",
    "\n",
    "'on the telephon then there were no cigarett and i went out to buy',\n",
    "\n",
    "'some at the drugstor on the corner when i came back they had' ...]\n",
    "\n",
    "y's = [\n",
    "0,\n",
    "\n",
    "0,\n",
    "\n",
    "2,...\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2019ac35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./data/processed/nb_processed_data.pkl', 'wb') as f:\n",
    "    pickle.dump((X_train, X_val, X_test, y_train, y_val, y_test), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b736b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4759a2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = CountVectorizer()\n",
    "\n",
    "# X_train = cv.fit_transform(X_train)\n",
    "# X_val = cv.transform(X_val)\n",
    "# X_test = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ff84b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849bf193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd38384",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
